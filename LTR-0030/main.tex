\documentclass{ximera}
\input{../preamble.tex}

\author{Anna Davis \and Paul Zachlin \and Paul Bender} \title{LTR-0030:  Composition and Inverses of Linear Transformations} \license{CC-BY 4.0}

\begin{document}
\begin{abstract}
  We define composition of linear transformations, inverse of a linear transformation, and discuss existence and uniqueness of inverses.
\end{abstract}
\maketitle

Note to Student:  In this module we will often use $U$, $V$ and $W$ to denote the domain and codomain of linear transformations.  If you are familiar with abstract vector spaces, you can regard $U$, $V$ and $W$ as finite-dimensional vector spaces, otherwise you may think of $U$, $V$ and $W$ as subspaces of $\RR^n$. 

\section*{LTR-0030:  Composition and Inverses of Linear Transformations}

\subsection*{Composition of Linear Transformations}
\begin{definition}\label{def:compoflintrans} Let $U$, $V$ and $W$ be vector spaces, and let $T:U\rightarrow V$ and $S:V\rightarrow W$ be linear transformations.  The \dfn{composition} of $S$ and $T$ is the transformation $S\circ T:U\rightarrow W$ given by
$$(S\circ T)(\vec{u})=S(T(\vec{u}))$$
\end{definition}

\begin{example}\label{ex:transcomp} Define $$T:\RR^2\rightarrow \RR^2 \quad\text{by}\quad T\left(\begin{bmatrix}u_1\\u_2\end{bmatrix}\right)=\begin{bmatrix}u_1+u_2\\3u_1+3u_2\end{bmatrix}$$

$$S:\RR^2\rightarrow \RR^2 \quad\text{by}\quad S\left(\begin{bmatrix}v_1\\v_2\end{bmatrix}\right)=\begin{bmatrix}3v_1-v_2\\-3v_1+v_2\end{bmatrix}$$
(You should be able to verify that both transformations are linear.)  Examine the effect of $S\circ T$ on vectors of $\RR^2$.
\begin{explanation}
From the computational standpoint, the situation is simple.
\begin{align*}
(S\circ T)\left(\begin{bmatrix}u_1\\u_2\end{bmatrix}\right)&=S\left(T\left(\begin{bmatrix}u_1\\u_2\end{bmatrix}\right)\right)=S\left(\begin{bmatrix}u_1+u_2\\3u_1+3u_2\end{bmatrix}\right)\\
&=\begin{bmatrix}3(u_1+u_2)-(3u_1+3u_2)\\-3(u_1+u_2)+(3u_1+3u_2)\end{bmatrix}\\
&=\vec{0}
\end{align*}
This means that $S\circ T$ maps all vectors of $\RR^2$ to $\vec{0}$.  

In addition to the computational approach, it is also useful to visualize what happens geometrically.

First, observe that $T\left(\begin{bmatrix}u_1\\u_2\end{bmatrix}\right)=\begin{bmatrix}u_1+u_2\\3u_1+3u_2\end{bmatrix}=(u_1+u_2)\begin{bmatrix}1\\3\end{bmatrix}$  So, the image of any vector of $\RR^2$ under $T$ lies on the line determined by the vector $\begin{bmatrix}1\\3\end{bmatrix}$.  

Even though $S$ is defined on all of $\RR^2$, we are only interested in the action of $S$ on vectors along the line determined by $\begin{bmatrix}1\\3\end{bmatrix}$.  Our computations showed that all such vectors map to $\vec{0}$.

The actions of individual transformations, as well as the composite transformation are shown below.

\begin{center}
\begin{tikzpicture}[scale=0.7]

\fill[blue!40] (-1,-1) rectangle (3,3);
\draw[thin,gray!40] (-1,-1) grid (3,3);
  \draw[<->] (-1,0)--(3,0);
  \draw[<->] (0,-1)--(0,3);

  \draw[thin,gray!40] (4,-1) grid (8,3);
  \draw[<->] (4,0)--(8,0);
  \draw[<->] (5,-1)--(5,3);
  \draw[line width=2pt,blue!40](14/3,-1)--(6,3) node[anchor=south west]{};
  
  \draw[thin,gray!40] (9,-1) grid (13,3);
  \draw[<->] (9,0)--(13,0);
  \draw[<->] (10,-1)--(10,3);
  
  \fill[blue!40!white] (10,0) circle (0.2cm);
  \draw [->,line width=1pt,-stealth]  (2,1)to[out=60, in=120](4.5, 1);
   \node[] at (3.4, 2)   (b) {$T$};
\draw [->,line width=1pt,-stealth] (7,1)to[out=60, in=120](9.5, 1);
\node[] at (8.4, 2)   (b) {$S$};
\draw [->,line width=1pt,-stealth] (2,3.5)to[out=30, in=150](11, 3.5);
\node[] at (6.5, 5.3)   (b) {$S\circ T$};
\end{tikzpicture}
\end{center}

\end{explanation}
\end{example}

\begin{theorem}\label{th:complinear}
The composition of two linear transformations is linear.
\end{theorem}
\begin{proof}
Let $T:U\rightarrow V$ and $S:V\rightarrow W$ be linear transformations. We will show that $S\circ T$ is linear.  For all vectors $\vec{u}_1$ and $\vec{u}_2$ of $U$ and scalars $a$ and $b$ we have:
\begin{align*}
(S\circ T)(a\vec{u}_1+b\vec{u}_2)&=S(T(a\vec{u}_1+b\vec{u}_2))\\
&=S(aT(\vec{u}_1)+bT(\vec{u}_2))\\
&=aS(T(\vec{u}_1))+bS(T(\vec{u}_2))\\
&=a(S\circ T)(\vec{u}_1)+b(S\circ T)(\vec{u}_2)
\end{align*}
\end{proof}

\begin{theorem}\label{th:compoflintransass} Composition of linear transformations is associative.  In other words, for linear transformations  $T$, $S$ and $R$

\begin{center}
\begin{tikzpicture}
\node{
\begin{tikzcd}
U\rar{T}\arrow[black, bend right]{rrr}[black,swap]{R\circ S\circ T}  & V \rar{S}  & W \rar{R} & Z
\end{tikzcd}
};
\end{tikzpicture}

\end{center}

We have $(R\circ S)\circ T=R\circ (S\circ T)$.
\end{theorem}
\begin{proof} For all $\vec{u}$ in $U$ we have:
\begin{align*}
((R\circ S)\circ T)(\vec{u})&=(R\circ S)(T(\vec{u}))=R(S(T(\vec{u})))\\
&=R((S\circ T)(\vec{u}))=(R\circ (S\circ T))(\vec{u})
\end{align*}
\end{proof}

\subsection*{Composition and Matrix Multiplication}

In this section we will consider linear transformations of $\RR^n$ and their standard matrices.  

\begin{theorem}\label{th:standardmatcompoflintrans}
Let $T:\RR^n\rightarrow \RR^m$ and $S:\RR^m\rightarrow \RR^p$ be linear transformations with standard matrices $M_T$ and $M_S$, respectively.  Then the composite transformation $S\circ T:\RR^n\rightarrow \RR^p$ has a standard matrix given by the product $M_SM_T$.
\end{theorem}
\begin{proof}
For all $\vec{v}$ in $\RR^n$ we have:
$$(S\circ T)(\vec{v})=S(T(\vec{v}))=S(M_T\vec{v})=M_S(M_T\vec{v})=(M_SM_T)\vec{v}$$
\end{proof}

\begin{example}\label{ex:standardmatofcomp}
In Example \ref{ex:transcomp}, we discussed a composite transformation $S\circ T:\RR^2\rightarrow \RR^2$
given by:
$$T\left(\begin{bmatrix}u_1\\u_2\end{bmatrix}\right)=\begin{bmatrix}u_1+u_2\\3u_1+3u_2\end{bmatrix}\quad \text{and} \quad
S\left(\begin{bmatrix}v_1\\v_2\end{bmatrix}\right)=\begin{bmatrix}3v_1-v_2\\-3v_1+v_2\end{bmatrix}$$
Express $S\circ T$ as a matrix transformation.
\begin{explanation}
The standard matrix for $T:\RR^2\rightarrow \RR^2$ is $$\begin{bmatrix}1&1\\3&3\end{bmatrix}$$ and the standard matrix for $S:\RR^2\rightarrow \RR^2$ is $$\begin{bmatrix}3&-1\\-3&1\end{bmatrix}$$
The standard matrix for $S\circ T$ is the product
$$\begin{bmatrix}3&-1\\-3&1\end{bmatrix}\begin{bmatrix}1&1\\3&3\end{bmatrix}=\begin{bmatrix}0&0\\0&0\end{bmatrix}$$
\end{explanation}
\end{example}

We conclude this section by revisiting the associative property of matrix multiplication.  At the time matrix multiplication was introduced, you might have found it cumbersome to prove that for appropriately sized matrices $A$, $B$ and $C$, we have $(AB)C=A(BC)$ (See Theorem \ref{th:propertiesofmatrixmultiplication} of MAT-0020).  We are now in a position to prove this result with ease.  

Every matrix induces a linear transformation.  The product of two matrices can be interpreted as a composition of transformations.  Since transformation composition is associative, so is matrix multiplication. We formalize this observation as a theorem.

\begin{theorem}[Associativity of Matrix Multiplication] \label{th:associativematrixmult}  Let $A$, $B$ and $C$ be matrices of appropriate dimensions so that the product $(AB)C$ is defined.  Then
$$ABC=(AB)C=A(BC)$$
\end{theorem}

\subsection*{Inverses of Linear Transformations}

\begin{exploration}\label{ep:inverse} Define a linear transformation $T:\RR^2\rightarrow \RR^2$ by $T(\vec{v})=2\vec{v}$.  In other words, $T$ doubles every vector in $\RR^2$.  Now define $S:\RR^2\rightarrow \RR^2$ by $S(\vec{v})=\frac{1}{2}\vec{v}$.  What happens when we compose these two transformations?
$$(S\circ T)(\vec{v})=S(T(\vec{v}))=S(2\vec{v})=\left(\frac{1}{2}\right)(2)\vec{v}=\vec{v}$$
$$(T\circ S)(\vec{v})=T(S(\vec{v}))=T(\frac{1}{2}\vec{v})=(2)\left(\frac{1}{2}\right)\vec{v}=\vec{v}$$
Both composite transformations return the original vector $\vec{v}$.  In other words, $S\circ T=\id_{\RR^2}$ and $T\circ S=\id_{\RR^2}$.  We say that $S$ is an \dfn{inverse} of $T$, and $T$ is an inverse of $S$.
\end{exploration}

\begin{definition}\label{def:inverseoflintrans} Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be a linear transformation.  A transformation $S:W\rightarrow V$ such that $S\circ T=\id_V$ and $T\circ S=\id_W$ is called an \dfn{inverse} of $T$. If $T$ has an inverse, $T$ is called \dfn{invertible}.
\end{definition}

\begin{example}\label{ex:inverseverify} Let $T:\RR^2\rightarrow \RR^2$ be a transformation defined by $T\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}x+y\\x-y\end{bmatrix}$. (How would you verify that $T$ is linear?)  Show that $S:\RR^2\rightarrow \RR^2$ given by $S\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}0.5x+0.5y\\0.5x-0.5y\end{bmatrix}$ is an inverse of $T$.
\begin{explanation}
We will show that $S\circ T=\id_{\RR^2}$.  
\begin{align*}
(S\circ T)\left(\begin{bmatrix}x\\y\end{bmatrix}\right)&=S\left(T\left(\begin{bmatrix}x\\y\end{bmatrix}\right)\right)=S\left(\begin{bmatrix}x+y\\x-y\end{bmatrix}\right)\\
&=\begin{bmatrix}0.5(x+y)+0.5(x-y)\\0.5(x+y)-0.5(x-y)\end{bmatrix}
=\begin{bmatrix}x\\y\end{bmatrix}
\end{align*}
We leave it to the reader to verify that $T\circ S=\id_{\RR^2}$.

\end{explanation}
\end{example}

\subsection*{Linearity of Inverses of Linear Transformations}

Definition \ref{def:inverseoflintrans} does not specifically require an inverse $S$ of a linear transformation $T$ to be linear, but it turns out that the requirement that $S\circ T=\id_V$ and $T\circ S=\id_W$ is sufficient to guarantee that $S$ is linear.  

\begin{theorem}\label{th:inverseislinear} Suppose $T:V\rightarrow W$ is an invertible linear transformation.  Let $S$ be an inverse of $T$.  Then $S$ is  linear.
\end{theorem}

\begin{proof} The proof of this result is left to the reader. (See Practice Problem \ref{prob:inverseislinear})
%Let $\vec{w}_1$ and $\vec{w}_2$ be vectors in $W$ such that $\vec{w}_1=T(\vec{v}_1)$ and $T(\vec{w}_2)=\vec{v}_2$ for some $\vec{v}_1$ and $\vec{v}_2$ in $V$. Let $k_1$ and $k_2$ be scalars.
%\begin{align*}
%S(k_1\vec{w}_1+k_2\vec{w}_2)&=S(k_1T(\vec{v}_1)+k_2T(\vec{v}_2))\\
%&=S(T(k_1\vec{v}_1+k_2\vec{v}_2))\\
%&=(S\circ T)(k_1\vec{v}_1+k_2\vec{v}_2)\\
%&=k_1\vec{v}_1+k_2\vec{v}_2\\
%&=k_1S(\vec{w}_1)+k_2S(\vec{w}_2)
%\end{align*}
%Therefore $S$ is linear.
\end{proof}


\subsection*{Linear Transformations of $\RR^n$ and the Standard Matrix of the Inverse Transformation}

Every linear transformation $T:\RR^n\rightarrow\RR^m$ is a matrix transformation. (See Theorem \ref{th:matlin} of LTR-0020)  If $T$ has an inverse $S$, then by Theorem \ref{th:inverseislinear}, $S$ is also a matrix transformation.  Let  $M_T$ and $M_S$ denote the standard matrices of $T$ and $S$, respectively.  We see that $S\circ T=\id_{\RR^n}$ and $T\circ S=\id_{\RR^m}$ if and only if $M_SM_T=I_{n}$ and $M_TM_S=I_{m}$.  In other words, $T$ and $S$ are inverse transformations if and only if $M_T$ and $M_S$ are matrix inverses.

Note that if $S$ is an inverse of $T$, then $M_T$ and $M_S$ are square matrices, and $n=m$. 

\begin{theorem}\label{th:existunique} Let $T:\RR^n\rightarrow \RR^n$ be a linear transformation, and let $M$ be the standard matrix of $T$.
  \begin{enumerate}
  \item \label{item:exists} {\bf Existence of Inverses.}  $T$ is invertible if and only if $M$ is invertible.  If $T$ is invertible, then the inverse is induced by $M^{-1}$.
  \item \label{item:unique}{\bf Uniqueness of Inverses.}  If $S$ is an inverse of $T$, then $S$ is unique.
  \end{enumerate}
\end{theorem}
\begin{proof}
Part \ref{item:exists} follows directly from the preceding discussion.  Part \ref{item:unique} follows from uniqueness of matrix inverses. (Theorem \ref{th:matinverseunique} of MAT-0050)
\end{proof}

Please note that Theorem \ref{th:existunique} is only applicable in the context of linear transformations of $\RR^n$ and their standard matrices.  The following example provides us with motivation to investigate inverses further, which we will do in LTR-0035.

\begin{exploration}\label{init:subtosub}
Let $$V=\text{span}\left(\begin{bmatrix}1\\0\\0\end{bmatrix}, \begin{bmatrix}1\\1\\1\end{bmatrix}\right)$$
Define a linear transformation $$T:V\rightarrow \RR^2$$
by $$T\left(\begin{bmatrix}1\\0\\0\end{bmatrix}\right)=\begin{bmatrix}1\\1\end{bmatrix}\quad \text{and} \quad T\left(\begin{bmatrix}1\\1\\1\end{bmatrix}\right)=\begin{bmatrix}0\\1\end{bmatrix}$$
Observe that $\left\{\begin{bmatrix}1\\0\\0\end{bmatrix}, \begin{bmatrix}1\\1\\1\end{bmatrix}\right\}$ is a basis of $V$ (why?). The information about the images of the basis vectors is sufficient to define a linear transformation. (See LTR-0025) %because every element $\vec{v}$ of $V$ can be written uniquely in the form $$\vec{v}=a\begin{bmatrix}1\\0\\0\end{bmatrix}+b\begin{bmatrix}1\\1\\1\end{bmatrix}$$  
%and our condition that $T$ is linear determines the image of $\vec{v}$ as follows: $$T(\vec{v})=a\begin{bmatrix}1\\1\end{bmatrix}+b\begin{bmatrix}0\\1\end{bmatrix}$$
So, at this point we know what transformation $T$ does, but it is still unclear what the matrix of this linear transformation is.

Geometrically speaking, the domain of $T$ is a plane in $\RR^3$ and its codomain is $\RR^2$.  

Does $T$ have an inverse?  We are not in a position to answer this question right now because Theorem \ref{th:existunique} does not apply to this situation. 
\end{exploration}
Exploration \ref{init:subtosub} highlights the necessity of a further discussion of inverses.  In Example \ref{ex:subtosub} of LTR-0035, we will prove that $T$ has an inverse, and in Example \ref{ex:inversematrixoftransform} of LTR-0080 we will relate the existence of an inverse of $T$ to a matrix associated with $T$.


\section*{Practice Problems}
\begin{problem}\label{prob:geometryoflintrans1}
Let $T:\RR^2\rightarrow \RR^2$ and $S:\RR^2\rightarrow \RR^2$ be linear transformations with standard matrices 
$$M_T=\begin{bmatrix}2&-4\\1&2\end{bmatrix}\quad\text{and}\quad M_S=\begin{bmatrix}1&-1\\2&1\end{bmatrix}$$
respectively.  Describe the actions of $T$, $S$, and $S\circ T$ geometrically, as in Example \ref{ex:transcomp}.
\end{problem}
\begin{problem}\label{prob:geometryoflintrans2}
Let $T:\RR^3\rightarrow \RR^2$ and $S:\RR^2\rightarrow \RR^2$ be linear transformations with standard matrices 
$$M_T=\begin{bmatrix}1&0&-1\\2&1&0\end{bmatrix}\quad\text{and}\quad M_S=\begin{bmatrix}-1&2\\1&-2\end{bmatrix}$$
respectively.  Describe the actions of $T$, $S$, and $S\circ T$ geometrically, as in Example \ref{ex:transcomp}.
\end{problem}
\begin{problem}\label{prob:completeinverseverify}
Complete the Explanation of Example \ref{ex:inverseverify} by verifying that $T\circ S=\id_{\RR^2}$.
\end{problem}
\begin{problem}\label{prob:candidateforinv}
Let $T:\RR^2\rightarrow \RR^2$ be a linear transformation given by 
$$T\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}2x-5y\\-x+3y\end{bmatrix}$$
Propose a candidate for an inverse of $T$ and verify your choice using Definition \ref{def:inverseoflintrans}.
\end{problem}
\begin{problem}\label{prob:noinversetrans}Explain why linear transformation $T:\RR^2\rightarrow\RR^2$ given by $$T\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}2x+2y\\-3x-3y\end{bmatrix}$$ does not have an inverse.
\end{problem}
\begin{problem}\label{prob:inverseislinear}
Prove Theorem \ref{th:inverseislinear}.  
\end{problem}
\begin{problem}\label{prob:compofinvisinvofcomp} Suppose $T:U\rightarrow V$ and $S:V\rightarrow W$ are linear transformations with inverses $T'$ and $S'$ respectively.  Prove that $T'\circ S'$ is an inverse of $S\circ T$.
\end{problem}

\end{document}

\documentclass{ximera}
\input{../preamble.tex}

\title{SVD Decomposition} \license{CC BY-NC-SA 4.0}

\begin{document}

\begin{abstract}
\end{abstract}
\maketitle

\section*{SVD Decomposition}
We begin this section with an important definition.

\begin{definition}\label{singularvalues}
Let $A$ be an $m\times n$ matrix. The \dfn{singular values} of $A$ are the square roots of the positive
eigenvalues of $A^TA.$ 
\end{definition}

Singular Value Decomposition (SVD) can be thought of as
a generalization of orthogonal diagonalization of a symmetric matrix
to an arbitrary $m\times n$ matrix. This decomposition is the focus of this section.

The following is a useful result that will help when computing the SVD of matrices.

\begin{lemma}\label{lem:samenonzeroeigenvalues}
Let $A$ be an $m \times n$ matrix. Then $A^TA$ and $AA^T$ have the same \textbf{nonzero eigenvalues}.
\end{lemma}

\begin{proof}
Suppose $A$ is an $m\times n$ matrix, and suppose that  $\lambda$ is a nonzero eigenvalue of $A^TA$.
Then there exists a nonzero vector $\vec{x} \in \RR^n$ such that
\begin{equation}\label{nonzero}
(A^TA)\vec{x}=\lambda \vec{x}
\end{equation}
Multiplying both sides of this equation by $A$ yields:
\begin{eqnarray*}
A(A^TA)\vec{x} & = & A\lambda \vec{x}\\
(AA^T)(A\vec{x}) & = & \lambda (A\vec{x})
\end{eqnarray*}
Since $\lambda\neq 0$ and $\vec{x}\neq 0_n$, $\lambda \vec{x}\neq 0_n$,
and thus by equation~(\ref{nonzero}),
$(A^TA)\vec{x}\neq 0_m$; thus $A^T(A\vec{x})\neq 0_m$,
implying that $A\vec{x}\neq 0_m$.

Therefore $A\vec{x}$ is an eigenvector of $AA^T$ corresponding to eigenvalue
$\lambda$.  An analogous argument can be used to show that every
nonzero eigenvalue of $AA^T$ is an eigenvalue of $A^TA$, thus
completing the proof.
\end{proof}

Given an $m\times n$ matrix $A$, we will see how to express $A$ as a product
\[ A=U\Sigma V^T\]
where
\begin{itemize}
\item $U$ is an $m\times m$ orthogonal matrix whose columns are
eigenvectors of $AA^T$.
\item $V$ is an $n\times n$ orthogonal matrix whose columns are
eigenvectors of $A^TA$.
\item $\Sigma$ is an $m\times n$ matrix whose only nonzero values
lie on its main diagonal, and are the singular values of $A$.
\end{itemize}
How can we find such a decomposition? We are aiming to decompose $A$ in the following form:
\begin{equation*}
A=U\left[
\begin{array}{cc}
\sigma & 0 \\
0 & 0
\end{array}
\right] V^T
\end{equation*}
where $\sigma $ is a block matrix of the form
\[
\sigma =\left[
\begin{array}{ccc}
\sigma _{1} &  & 0 \\
& \ddots &  \\
0 &  & \sigma _{k}
\end{array}
\right]
\]
Thus $A^T=V\left[
\begin{array}{cc}
\sigma & 0 \\
0 & 0
\end{array}
\right] U^T$ and it follows that
\begin{equation*}
A^TA=V\left[
\begin{array}{cc}
\sigma & 0 \\
0 & 0
\end{array}
\right] U^TU\left[
\begin{array}{cc}
\sigma & 0 \\
0 & 0
\end{array}
\right] V^T=V\left[
\begin{array}{cc}
\sigma ^{2} & 0 \\
0 & 0
\end{array}
\right] V^T
\end{equation*}
and so $A^TAV=V\left[
\begin{array}{cc}
\sigma ^{2} & 0 \\
0 & 0
\end{array}
\right] .$ Similarly, $AA^TU=U\left[
\begin{array}{cc}
\sigma ^{2} & 0 \\
0 & 0
\end{array}
\right] .$ Therefore, you would find an orthonormal basis of eigenvectors
for $AA^T$ make them the columns of a matrix such that the
corresponding eigenvalues are decreasing. This gives $U.$ You could then do
the same for $A^TA$ to get $V$.

We formalize this discussion in the following theorem.

\begin{theorem}[Singular Value Decomposition]\label{th:singvaldecomp}
Let $A$ be an $m\times n$ matrix. Then there exist
orthogonal matrices $U$ and $V$ of the appropriate size such that $A= U \Sigma V^T$ where $\Sigma$ is of the form
\[
\Sigma =
\left[
\begin{array}{cc}
\sigma & 0 \\
0 & 0
\end{array}
\right]
\]
and $\sigma $ is a block matrix of the form
\[
\sigma =\left[
\begin{array}{ccc}
\sigma _{1} &  & 0 \\
& \ddots &  \\
0 &  & \sigma _{k}
\end{array}
\right]
\]
for the $\sigma _{i}$ the singular values of $A.$
\end{theorem}

\begin{proof}
There exists an orthonormal basis, $\left\{ \vec{v}_{1}, \dots, \vec{v}_n\right\}$ such that $
A^TA\vec{v}_{i}=\sigma _{i}^{2}\vec{v}_{i}$ where $\sigma
_{i}^{2}>0$ for $i=1,\dots ,k,\left( \sigma _{i}>0\right) $ and equals zero
if $i>k.$ Thus for $i>k,$ $A\vec{v}_{i}=\vec{0}$ because
\begin{equation*}
 A\vec{v}_{i}\dotp A\vec{v}_{i} = (A\vec{v}_i)^T(A\vec{v}_i)=\vec{v}_i^T(A^TA\vec{v}_i)=\vec{0}
 %A^TA\vec{v}_{i} \dotp \vec{v}_{i}  = \vec{0} \dotp \vec{v}_{i} =0
\end{equation*}
For $i=1,\dots ,k,$ define $\vec{u}_{i}\in \RR^{m}$ by
\begin{equation*}
\vec{u}_{i}= \sigma _{i}^{-1}A\vec{v}_{i}
\end{equation*}
Thus $A\vec{v}_{i}=\sigma _{i}\vec{u}_{i}.$ Now
\begin{eqnarray*}
\vec{u}_{i} \dotp \vec{u}_{j} &=&  \sigma _{i}^{-1}A
\vec{v}_{i} \dotp \sigma _{j}^{-1}A\vec{v}_{j}  = \sigma_{i}^{-1}\vec{v}_{i}^T \sigma _{j}^{-1}A^TA\vec{v}_{j} \\
&=& \sigma _{i}^{-1}\vec{v}_{i} \dotp \sigma _{j}^{-1}\sigma _{j}^{2} \vec{v}_{j} =
\frac{\sigma _{j}}{\sigma _{i}}\left( \vec{v}_{i} \dotp \vec{v}_{j}\right)%=\delta _{ij} 
\end{eqnarray*}
This means that $\vec{u}_{i} \dotp \vec{u}_{j}=1$ when $i=j$ and $\vec{u}_{i} \dotp \vec{u}_{j}=0$ when $i\neq j$.
Thus $\left\{ \vec{u}_{1}, \dots, \vec{u}_{k}\right\}$ is an orthonormal set of
vectors in $\RR^{m}.$ Also,
\begin{equation*}
AA^T\vec{u}_{i}=AA^T\sigma _{i}^{-1}A\vec{v}_{i}=\sigma
_{i}^{-1}AA^TA\vec{v}_{i}=\sigma _{i}^{-1}A\sigma _{i}^{2}\vec{v}
_{i}=\sigma _{i}^{2}\vec{u}_{i}
\end{equation*}
Now, using Gram-Schmidt, extend $\left\{ \vec{u}_{1}, \dots, \vec{u}_{k}\right\}$ to an orthonormal
basis for all of $\RR^{m},\left\{ \vec{u}_{1},\dots,\vec{u}_{m}\right\}$
and let
\begin{equation*}
U= \left[
\begin{array}{ccc}
\vec{u}_{1} & \cdots & \vec{u}_{m}
\end{array}
\right]
\end{equation*}
while $V= \left[ \begin{array}{ccc} \vec{v}_{1} & \cdots & \vec{v}_{n}\end{array}\right] .$ Thus $U$
is the matrix which has the $\vec{u}_{i}$ as columns and $V$ is defined
as the matrix which has the $\vec{v}_{i}$ as columns. Then
\begin{equation*}
U^TAV=\left[
\begin{array}{c}
\vec{u}_{1}^T \\
\vdots \\
\vec{u}_{k}^T \\
\vdots \\
\vec{u}_{m}^T
\end{array}
\right] A  \left[ \begin{array}{ccc} \vec{v}_{1} & \cdots & \vec{v}_{n}\end{array}\right]
\end{equation*}
\begin{equation*}
=\left[
\begin{array}{c}
\vec{u}_{1}^T \\
\vdots \\
\vec{u}_{k}^T \\
\vdots \\
\vec{u}_{m}^T
\end{array}
\right] \left[
\begin{array}{cccccc}
\sigma _{1}\vec{u}_{1} & \cdots & \sigma _{k}\vec{u}_{k} & \vec{0}
& \cdots & \vec{0}
\end{array}
\right] =\left[
\begin{array}{cc}
\sigma & 0 \\
0 & 0
\end{array}
\right]
\end{equation*}
where $\sigma $ is given in the statement of the theorem.
\end{proof}

The SVD has as an immediate corollary which is given in the following interesting result.

\begin{corollary}\label{cor:ranksingularvalues}
Let $A$ be an $m\times n$ matrix. Then the rank of $A$ (or of $A^T$) equals
the number of singular values.
\end{corollary}

%%\begin{proof}
%%Since $V$ and $U$ are unitary, it follows that
%%\begin{eqnarray*}
%%\limfunc{rank}\left( A\right) &=&\limfunc{rank}\left( U^TAV\right) =
%%\limfunc{rank}\left[
%%\begin{array}{cc}
%%\sigma & 0 \\
%%0 & 0
%%\end{array}
%%\right] \\
%%&=&\text{number of singular values.}
%%\end{eqnarray*}
%%Also since $U,V$ are unitary,
%%\begin{equation*}
%%\func{rank}\left( A^T\right) =\func{rank}\left( V^TA^{\ast
%%}U\right) =\func{rank}\left( \left( U^TAV\right) ^T\right)
%%\end{equation*}
%%\begin{equation*}
%%=\func{rank}\left( \left[
%%\begin{array}{cc}
%%\sigma & 0 \\
%%0 & 0
%%\end{array}
%%\right] ^T\right) =\text{number of singular values.}
%%\end{equation*}
%%\end{proof}
%%
%%\medskip

Let's compute the SVD of a simple matrix.

\begin{example}\label{ex:SVD2x3}
Let
$A=\left[\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\right]$.
Find the SVD of $A$.

\begin{explanation}
To begin, we compute $AA^T$ and $A^TA$.
\[ AA^T = \left[\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\right]
\left[\begin{array}{rr} 1 & 3 \\ -1 & 1 \\ 3 & 1  \end{array}\right]
= \left[\begin{array}{rr} 11 & 5 \\ 5 & 11  \end{array}\right]\]

\[ A^TA = \left[\begin{array}{rr} 1 & 3 \\ -1 & 1 \\ 3 & 1  \end{array}\right]
\left[\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\right]
= \left[\begin{array}{rrr} 10 & 2 & 6 \\ 2 & 2 & -2\\
6 & -2 & 10 \end{array}\right]\]
Since $AA^T$ is $2\times 2$ while $A^T A$ is $3\times 3$, and $AA^T$
and $A^TA$ have the same {\em nonzero} eigenvalues (by Lemma
\ref{lem:samenonzeroeigenvalues}), we compute the characteristic polynomial  $c_{AA^T}(x)$ (because it is
easier to compute than $c_{A^TA}(x)$).
\begin{eqnarray*}
c_{AA^T}(z)& = &\det(zI-AA^T)=  \det \left[\begin{array}{cc}
z-11 & -5 \\ -5 & z-11 \end{array}\right]\\
& = &(z-11)^2 - 25 \\
& = & z^2-22z+121-25\\
& = & z^2-22z+96\\
& = & (z-16)(z-6)
\end{eqnarray*}
Therefore, the eigenvalues of $AA^T$ are $\lambda_1=16$ and $\lambda_2=6$.

The eigenvalues of $A^TA$ are $\lambda_1=16$, $\lambda_2=6$, and
$\lambda_3=0$, and the singular values of $A$ are $\sigma_1=\sqrt{16}=4$ and
$\sigma_2=\sqrt{6}$.
By convention, we list the eigenvalues (and corresponding singular values)
in non increasing order (i.e., from largest to smallest).

\textbf{To find the matrix $V$}:

To construct the matrix $V$ we need to find eigenvectors for $A^TA$.
Since the eigenvalues of $AA^T$ are distinct, the corresponding
eigenvectors are orthogonal, and we need only normalize them.

$\lambda_1=16$: solve $(16I-A^TA)\vec{x}_1= \vec{0}$.

\[ \left[\begin{array}{rrr|r}
6 & -2 & -6 & 0 \\ -2 & 14 & 2 & 0 \\ -6 & 2 & 6 & 0
\end{array}\right]
\rightarrow
\left[\begin{array}{rrr|r}
1 & 0 & -1 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0
\end{array}\right],
\mbox{ so }
\vec{x}_1 =\left[\begin{array}{r} t \\ 0 \\ t \end{array}\right]
=t\left[\begin{array}{r} 1 \\ 0 \\ 1 \end{array}\right],
t\in \RR. \]

$\lambda_2=6$: solve $(6I-A^TA)\vec{x}_2= \vec{0}$.

\[ \left[\begin{array}{rrr|r}
-4 & -2 & -6 & 0 \\ -2 & 4 & 2 & 0 \\ -6 & 2 & -4 & 0
\end{array}\right]
\rightarrow
\left[\begin{array}{rrr|r}
1 & 0 & 1 & 0 \\ 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 0
\end{array}\right],
\mbox{ so }
\vec{x}_2=\left[\begin{array}{r} -s \\ -s \\ s \end{array}\right]
=s\left[\begin{array}{r} -1 \\ -1 \\ 1 \end{array}\right],
s\in \RR. \]

$\lambda_3=0$: solve $(-A^TA)\vec{x}_3= \vec{0}$.
\[ \left[\begin{array}{rrr|r}
-10 & -2 & -6 & 0 \\ -2 & -2 & 2 & 0 \\ -6 & 2 & -10 & 0
\end{array}\right]
\rightarrow
\left[\begin{array}{rrr|r}
1 & 0 & 1 & 0 \\ 0 & 1 & -2 & 0 \\ 0 & 0 & 0 & 0
\end{array}\right],
\mbox{ so }
\vec{x}_3=\left[\begin{array}{r} -r \\ 2r \\ r \end{array}\right]
=r\left[\begin{array}{r} -1 \\ 2 \\ 1 \end{array}\right],
r\in \RR. \]
Let
\[ \vec{v}_1=\frac{1}{\sqrt{2}}\left[\begin{array}{r} 1\\ 0\\ 1 \end{array}\right],
\vec{v}_2=\frac{1}{\sqrt{3}}\left[\begin{array}{r} -1\\ -1\\ 1 \end{array}\right],
\vec{v}_3=\frac{1}{\sqrt{6}}\left[\begin{array}{r} -1\\ 2\\ 1 \end{array}\right]\]
Then
\[ V=\frac{1}{\sqrt{6}}\left[\begin{array}{rrr}
\sqrt 3 & -\sqrt 2 & -1  \\
0 & -\sqrt 2 & 2 \\
\sqrt 3 & \sqrt 2 & 1 \end{array}\right]\]
Also,
\[ \Sigma = \left[\begin{array}{rrr} 4 & 0 & 0 \\
0 & \sqrt 6 & 0 \end{array}\right]\]
and we use $A$, $V^T$, and $\Sigma$ to find $U$. Since $V$ is orthogonal and $A=U\Sigma V^T$, it follows that $AV=U\Sigma$.
Let $V=\left[\begin{array}{ccc} \vec{v}_1 & \vec{v}_2 & \vec{v}_3 \end{array}\right]$, and
let $U=\left[\begin{array}{cc} \vec{u}_1 & \vec{u}_2 \end{array}\right]$, where
$\vec{u}_1$ and $\vec{u}_2$ are the two columns of $U$. Then we have
\begin{eqnarray*}
A\left[\begin{array}{ccc} \vec{v}_1 & \vec{v}_2 & \vec{v}_3 \end{array}\right]
&=& \left[\begin{array}{cc} \vec{u}_1 & \vec{u}_2 \end{array}\right]\Sigma\\
\left[\begin{array}{ccc} A\vec{v}_1 & A\vec{v}_2 & A\vec{v}_3 \end{array}\right]
&=& \left[\begin{array}{ccc} \sigma_1\vec{u}_1 + 0\vec{u}_2 &
0\vec{u}_1 + \sigma_2 \vec{u}_2 & 0 \vec{u}_1 + 0\vec{u}_2 \end{array}\right] \\
&=& \left[\begin{array}{ccc} \sigma_1\vec{u}_1 & \sigma_2 \vec{u}_2 &
0 \end{array}\right]
\end{eqnarray*}
which implies that $A\vec{v}_1=\sigma_1\vec{u}_1 = 4\vec{u}_1$ and
$A\vec{v}_2=\sigma_2\vec{u}_2 = \sqrt 6 \vec{u}_2$. Thus,
\[ \vec{u}_1 = \frac{1}{4}A\vec{v}_1
= \frac{1}{4}
\left[\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\right]
\frac{1}{\sqrt{2}}\left[\begin{array}{r} 1\\ 0\\ 1 \end{array}\right]
= \frac{1}{4\sqrt 2}\left[\begin{array}{r} 4\\ 4 \end{array}\right]
= \frac{1}{\sqrt 2}\left[\begin{array}{r} 1\\ 1 \end{array}\right]\]
and
\[ \vec{u}_2 = \frac{1}{\sqrt 6}A\vec{v}_2
= \frac{1}{\sqrt 6}
\left[\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\right]
\frac{1}{\sqrt{3}}\left[\begin{array}{r} -1\\ -1\\ 1 \end{array}\right]
=\frac{1}{3\sqrt 2}\left[\begin{array}{r} 3\\ -3 \end{array}\right]
=\frac{1}{\sqrt 2}\left[\begin{array}{r} 1\\ -1 \end{array}\right]
\]
Therefore,
\[ U=\frac{1}{\sqrt{2}}\left[\begin{array}{rr} 1 & 1 \\
1 & -1 \end{array}\right]\]
and
\begin{eqnarray*}
A & = & \left[\begin{array}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{array}\right]\\
& = & \left(\frac{1}{\sqrt{2}}\left[\begin{array}{rr} 1 & 1 \\
1 & -1 \end{array}\right]\right)
\left[\begin{array}{rrr} 4 & 0 & 0 \\
0 & \sqrt 6 & 0 \end{array}\right]
\left(\frac{1}{\sqrt{6}}\left[\begin{array}{rrr}
\sqrt 3 & 0 & \sqrt 3  \\
-\sqrt 2 & -\sqrt 2 & \sqrt2 \\
-1 & 2 & 1 \end{array}\right]\right)
\end{eqnarray*}
\end{explanation}
\end{example}


Here is another example.

\begin{example}\label{ex:SVD3x1}
Find an SVD for
$A=\left[\begin{array}{r} -1 \\ 2\\ 2 \end{array}\right]$.

\begin{explanation}
Since $A$ is $3\times 1$, $A^T A$ is a $1\times 1$ matrix
whose eigenvalues are easier to find than the eigenvalues of
the $3\times 3$ matrix $AA^T$.

\[ A^TA=\left[\begin{array}{ccc} -1 & 2 & 2 \end{array}\right]
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right]
=\left[\begin{array}{r} 9 \end{array}\right]\]
Thus $A^TA$ has eigenvalue $\lambda_1=9$, and
the eigenvalues of $AA^T$ are $\lambda_1=9$, $\lambda_2=0$, and
$\lambda_3=0$.
Furthermore, $A$ has only one singular value, $\sigma_1=3$.

\textbf{To find the matrix $V$}:
To do so we find an eigenvector for $A^TA$ and
normalize it.
In this case, finding a unit eigenvector is trivial:
$\vec{v}_1=\left[\begin{array}{r} 1 \end{array}\right]$, and
\[ V=\left[\begin{array}{r} 1 \end{array}\right]\]
Also,
$\Sigma =\left[\begin{array}{r} 3 \\ 0\\ 0 \end{array}\right]$,
and we use $A$, $V^T$, and $\Sigma$ to find $U$.

\noindent Now $AV=U\Sigma$, with
$V=\left[\begin{array}{r}\vec{v}_1 \end{array}\right]$,
and $U=\left[\begin{array}{rrr} \vec{u}_1 & \vec{u}_2 & \vec{u}_3 \end{array}\right]$,
where $\vec{u}_1$, $\vec{u}_2$, and $\vec{u}_3$ are the columns of $U$.
Thus
\begin{eqnarray*}
A\left[\begin{array}{r} \vec{v}_1 \end{array}\right]
&=& \left[\begin{array}{rrr} \vec{u}_1 & \vec{u}_2 & \vec{u}_3 \end{array}\right]\Sigma\\
\left[\begin{array}{r} A\vec{v}_1 \end{array}\right]
&=& \left[\begin{array}{r} \sigma_1 \vec{u}_1+0\vec{u}_2+0\vec{u}_3 \end{array}\right]\\
&=& \left[\begin{array}{r} \sigma_1 \vec{u}_1 \end{array}\right]
\end{eqnarray*}
This gives us $A\vec{v}_1=\sigma_1 \vec{u}_1= 3\vec{u}_1$, so

\[ \vec{u}_1 = \frac{1}{3}A\vec{v}_1
= \frac{1}{3}
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right]
\left[\begin{array}{r} 1 \end{array}\right]
= \frac{1}{3}
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right]\]
The vectors $\vec{u}_2$ and $\vec{u}_3$ are eigenvectors of $AA^T$ corresponding
to the eigenvalue $\lambda_2=\lambda_3=0$.
Instead of solving the system $(0I-AA^T)\vec{x}= 0$ and then using the
Gram-Schmidt process on the resulting set of
two basic eigenvectors, the following approach may be used.


Find vectors $\vec{u}_2$ and $\vec{u}_3$ by first extending $\{ \vec{u}_1\}$ to a basis of
$\RR^3$, then using the Gram-Schmidt algorithm to orthogonalize the basis,
and finally normalizing the vectors.

Starting with $\{ 3\vec{u}_1 \}$ instead of $\{ \vec{u}_1 \}$ makes the
arithmetic a bit easier.
It is easy to verify that

\[ \left\{ \left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right],
\left[\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\right],
\left[\begin{array}{r} 0 \\ 1 \\ 0 \end{array}\right]\right\}\]
is a basis of $\RR^3$.  Set

\[ \vec{x}_1 = \left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right],
\vec{x}_2 = \left[\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\right],
\vec{x}_3 =\left[\begin{array}{r} 0 \\ 1 \\ 0 \end{array}\right]\]
and apply the Gram-Schmidt algorithm to
$\{ \vec{x}_1, \vec{x}_2, \vec{x}_3\}$. This gives us

\[ \vec{f}_1 = \left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right], \vec{f}_2 = \left[\begin{array}{r} 4 \\ 1 \\ 1 \end{array}\right]
\mbox{ and }
\vec{f}_3 = \left[\begin{array}{r} 0 \\ 1 \\ -1 \end{array}\right]\]
Therefore,
\[ \vec{u}_2 = \frac{1}{\sqrt{18}}
 \left[\begin{array}{r} 4 \\ 1 \\ 1 \end{array}\right],
\vec{u}_3 = \frac{1}{\sqrt 2}
\left[\begin{array}{r} 0 \\ 1 \\ -1 \end{array}\right]\]
and
\[ U = \left[  \begin{array}{rrr} -\frac{1}{3} & \frac{4}{\sqrt{18}} & 0 \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & \frac{1}{\sqrt 2} \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & -\frac{1}{\sqrt 2} \end{array}\right]\]
Finally,
\[ A =
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right]
=
\left[ \begin{array}{rrr} -\frac{1}{3} & \frac{4}{\sqrt{18}} & 0 \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & \frac{1}{\sqrt 2} \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & -\frac{1}{\sqrt 2} \end{array}\right]
\left[\begin{array}{r} 3 \\ 0 \\ 0 \end{array}\right]
\left[\begin{array}{r} 1 \end{array}\right]\]

\end{explanation}
\end{example}

Consider another example.

\begin{example}\label{SVDanother2x3}
Find an SVD for the matrix
\begin{equation*}
A= \left[
\begin{array}{ccc}
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0 \\
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0
\end{array}
\right]
\end{equation*}

\begin{explanation}
First consider $A^TA$
\begin{equation*}
\left[
\begin{array}{ccc}
\frac{16}{5} & \frac{32}{5} & 0 \\
\frac{32}{5} & \frac{64}{5} & 0 \\
0 & 0 & 0
\end{array}
\right]
\end{equation*}
What are some eigenvalues and eigenvectors? Some computing shows that the eigenvalues are $0$ and $16$.  Furthermore, we can find a basis for each eigenspace.

\begin{equation*}
\mathcal{S}_0=\mbox{span}\left( \left[
\begin{array}{c}
0 \\
0 \\
1
\end{array}
\right] ,\left[  
\begin{array}{c}
-\frac{2}{5}\sqrt{5} \\
\frac{1}{5}\sqrt{5} \\
0
\end{array}
\right] \right),
\quad\mathcal{S}_{16}=\mbox{span}\left( \left[  
\begin{array}{c}
\frac{1}{5}\sqrt{5} \\
\frac{2}{5}\sqrt{5} \\
0
\end{array}
\right] \right) 
\end{equation*}
Thus the matrix $V$ is given by
\begin{equation*}
V=\left[
\begin{array}{ccc}
\frac{1}{5}\sqrt{5} & -\frac{2}{5}\sqrt{5} & 0 \\
\frac{2}{5}\sqrt{5} & \frac{1}{5}\sqrt{5} & 0 \\
0 & 0 & 1
\end{array}
\right]
\end{equation*}
Next consider $AA^T$
\begin{equation*}
\left[
\begin{array}{cc}
8 & 8 \\
8 & 8
\end{array}
\right]
\end{equation*}
Eigenvalues are $0$ and $16$, and eigenspaces are

\begin{equation*}
\mathcal{S}_0=\mbox{span}\left(\left[  
\begin{array}{c}
-\frac{1}{2}\sqrt{2} \\
\frac{1}{2}\sqrt{2}
\end{array}
\right] \right),\quad\mathcal{S}_{16}=\mbox{span}\left( \left[  
\begin{array}{c}
\frac{1}{2}\sqrt{2} \\
\frac{1}{2}\sqrt{2}
\end{array}
\right] \right) 
\end{equation*}
Thus you can let $U$ be given by
\begin{equation*}
U=\left[  
\begin{array}{cc}
\frac{1}{2}\sqrt{2} & -\frac{1}{2}\sqrt{2} \\
\frac{1}{2}\sqrt{2} & \frac{1}{2}\sqrt{2}
\end{array}
\right]
\end{equation*}
Let's check this. $U^TAV=$
\begin{equation*}
\left[  
\begin{array}{cc}
\frac{1}{2}\sqrt{2} & \frac{1}{2}\sqrt{2} \\
-\frac{1}{2}\sqrt{2} & \frac{1}{2}\sqrt{2}
\end{array}
\right] \left[   
\begin{array}{ccc}
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0 \\
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0
\end{array}
\right] \left[ 
\begin{array}{ccc}
\frac{1}{5}\sqrt{5} & -\frac{2}{5}\sqrt{5} & 0 \\
\frac{2}{5}\sqrt{5} & \frac{1}{5}\sqrt{5} & 0 \\
0 & 0 & 1
\end{array}
\right]
\end{equation*}
\begin{equation*}
=\left[
\begin{array}{ccc}
4 & 0 & 0 \\
0 & 0 & 0
\end{array}
\right]
\end{equation*}
\end{explanation}
\end{example}

This illustrates that if you have a good way to find the eigenvectors and
eigenvalues for a Hermitian matrix which has nonnegative eigenvalues, then
you also have a good way to find the SVD of an
arbitrary matrix.



\section*{Text Source} This section was adapted from Section 8.11 of Keith Nicholson's \href{https://open.umn.edu/opentextbooks/textbooks/linear-algebra-with-applications}{\it Linear Algebra with Applications}. (CC-BY-NC-SA)

W. Keith Nicholson, {\it Linear Algebra with Applications}, Lyryx 2018, Open Edition.
\end{document}
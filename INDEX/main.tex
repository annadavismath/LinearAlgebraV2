\documentclass{ximera}
\input{../preamble.tex}

\title{INDEX} \license{CC BY-NC-SA 4.0}



\begin{document}
\begin{abstract}
\end{abstract}
\maketitle



\section{INDEX}
A hyperlinked term will take you to the section where the term is defined.  Parenthetical hyperlink will take you to the specific definition or formula in the section.  Use arrows on the right to display the definition or formula in the index.
\subsection{A}
\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VEC-0030/main}{addition of vectors}

%adjoint of a matrix

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/DET-0060/main}{adjugate of a matrix}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/EIG-0020/main}{algebraic multiplicity of an eigenvalue}

associated homogeneous system (\ref{def:asshomsys})
\begin{expandable}
    Given any linear system $A\vec{x}=\vec{b}$, the system $A\vec{x}=\vec{0}$ is called the \dfn{associated homogeneous system}.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0020/main}{augmented matrix}


\subsection{B}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0020/main}{back substitution}

basis (\ref{def:basis})
\begin{expandable}
    A set $\mathcal{S}$ of vectors is called a \dfn{basis} of $\RR^n$ (or a basis of a subspace $V$ of $\RR^n$) provided that 
\begin{enumerate}
\item 
$\mbox{span}(\mathcal{S})=\RR^n$ (or $V$)
\item 
$\mathcal{S}$ is linearly independent.
\end{enumerate}
\end{expandable}

%Basis Theorem

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0023/main}{block matrices}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/DET-0070/main}{box product}

\subsection{C}

change-of-basis matrix (\ref{def:matlintransgenera})
\begin{expandable}
    Matrix $A$ of Theorem \ref{th:matlintransgeneral} is called the matrix of $T$ with respect to ordered bases $\mathcal{B}$ and $\mathcal{C}$.
\end{expandable}

characteristic equation (\ref{def:chareqcharpoly})
\begin{expandable}
    The equation 
$$\mbox{det}(A-\lambda I) = 0$$ is called the \dfn{characteristic equation} of $A$. 
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/EIG-0020/main}{characteristic polynomial}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/RTH-0045/main}{Cholesky factorization}

closed under addition (\ref{def:closedunderaddition})
\begin{expandable}
    A set $V$ is said to be \dfn{closed under addition} if for each element $\vec{u} \in V$ and $\vec{v} \in V$ the sum $\vec{u}+\vec{v}$ is also in $V$.
\end{expandable}

closed under scalar multiplication (\ref{def:closedunderscalarmult})
\begin{expandable}
    A set $V$ is said to be \dfn{closed under scalar multiplication} if for each element $\vec{v} \in V$  and for each scalar $k \in \RR$ the product $k\vec{v}$ is also in $V$.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/LTR-0010/main}{codomain of a linear transformation}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0020/main}{coefficient matrix}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/DET-0010/main}{cofactor expansion}

%cofactor of a square matrix %see 7.6 LaPlace Expansion

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0010/main}{column matrix (vector)}

column space of a matrix (\ref{def:colspace})
\begin{expandable}
    Let $A$ be an $m\times n$ matrix.  The \dfn{column space} of $A$, denoted by $\mbox{col}(A)$, is the subspace of $\RR^m$ spanned by the columns of $A$.
\end{expandable}

composition of linear transformations (\ref{def:compoflintrans})
\begin{expandable}
    Let $U$, $V$ and $W$ be vector spaces, and let $T:U\rightarrow V$ and $S:V\rightarrow W$ be linear transformations.  The \dfn{composition} of $S$ and $T$ is the transformation $S\circ T:U\rightarrow W$ given by
$$(S\circ T)(\vec{u})=S(T(\vec{u}))$$
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0010/main}{consistent system}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0040/main}{convergence}

coordinate vector with respect to a basis (\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VSP-0030/main}{in $\RR^n$}) (\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VSP-0060/main}{abstract vector spaces})

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/DET-0060/main}{Cramer's Rule}

cross product (\ref{def:crossproduct})
\begin{expandable}
    Let $\vec{u=\begin{bmatrix}u_1\\u_2\\u_3\end{bmatrix}}$ and $\vec{v}=\begin{bmatrix}v_1\\v_2\\v_3\end{bmatrix}$ be vectors in $\RR^3$.  The \dfn{cross product} of $\vec{u}$ and $\vec{v}$, denoted by $\vec{u}\times\vec{v}$, is given by
$$\vec{u}\times\vec{v}=(u_2v_3-u_3v_2)\vec{i}-(u_1v_3-u_3v_1)\vec{j}+(u_1v_2-u_2v_1)\vec{k}
=\begin{bmatrix}u_2v_3-u_3v_2\\-u_1v_3+u_3v_1\\u_1v_2-u_2v_1\end{bmatrix}$$
\end{expandable}

\subsection{D}
determinant ($2\times 2$) (\ref{def:twodetcrossprod})
\begin{expandable}
    A $2\times 2$ \dfn{determinant} is a number associated with a $2\times 2$ matrix

$$\det{\begin{bmatrix}
a & b\\
c & d
\end{bmatrix}}=\begin{vmatrix}
a & b\\
c & d
\end{vmatrix} =ad-bc$$
\end{expandable}

determinant ($3\times 3$) (\ref{def:threedetcrossprod})
\begin{expandable}
    A $3\times 3$ \dfn{determinant} is a number associated with a $3\times 3$ matrix
$$\det{\begin{bmatrix}
a_1 & a_2 & a_3\\
b_1 & b_2 &b_3\\
c_1 &c_2 &c_3
\end{bmatrix}}=
\begin{vmatrix}
a_1 & a_2 & a_3\\
b_1 & b_2 &b_3\\
c_1 &c_2 &c_3
\end{vmatrix} =a_1
\begin{vmatrix}
b_2 & b_3\\
c_2 & c_3
\end{vmatrix} -a_2
\begin{vmatrix}
b_1 & b_3\\
c_1 & c_3
\end{vmatrix} +a_3
\begin{vmatrix}
b_1 & b_2\\
c_1 & c_2
\end{vmatrix}
$$
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0025/main}{diagonal matrix}

diagonalizable matrix (\ref{def:diagonalizable})
\begin{expandable}
    Let $A$ be an $n\times n$ matrix. Then $A$ is said to be \dfn{diagonalizable} if there exists an invertible matrix $P$ such that
\begin{equation*}
P^{-1}AP=D
\end{equation*}
where $D$ is a diagonal matrix.  In other words, a matrix $A$ is diagonalizable if it is similar to a diagonal matrix, $A \sim D$.
\end{expandable}

dimension (\ref{def:dimension}) (also see \ref{def:dimensionabstract})
\begin{expandable}
    Let $V$ be a subspace of $\RR^n$.  The \dfn{dimension} of $V$ is the number, $m$, of elements in any basis of $V$.  We write
$$\mbox{dim}(V)=m$$
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/RRN-0020/main}{Direction vector}

Distance between points in $\RR^n$ (\ref{form:distRn})
\begin{expandable}
%\begin{formula}
Let $A(a_1, a_2,\ldots ,a_n)$ and $B(b_1, b_2,\ldots ,b_n)$ be points in $\RR^n$.  The distance between $A$ and $B$ is given by
$$AB=\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+\ldots +(a_n-b_n)^2}$$
%\end{formula}
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VEC-0070/main}{Distance between point and line}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0040/main}{divergence}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/LTR-0010/main}{domain of a linear transformation} 

dominant eigenvalue (\ref{def:dominant ew,ev})
\begin{expandable}
    An eigenvalue $\lambda$ of an $n \times n$ matrix $A$ is called a \dfn{dominant eigenvalue} if $\lambda$ has multiplicity $1$, and
\begin{equation*}
|\lambda| > |\mu| \quad \mbox{ for all eigenvalues } \mu \neq \lambda
\end{equation*}
Any corresponding eigenvector is called a \dfn{dominant eigenvector} of $A$.
\end{expandable}

dot product (\ref{def:dotproduct})
\begin{expandable}
 %   \begin{definition}
  Let $\vec{u}$ and $\vec{v}$ be vectors in $\RR^n$.  The \dfn{dot
    product} of $\vec{u}$ and $\vec{v}$, denoted by
  $\vec{u}\dotp \vec{v}$, is given by
$$\vec{u}\dotp\vec{v}=\begin{bmatrix}u_1\\u_2\\\vdots\\u_n\end{bmatrix}\dotp\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}=u_1v_1+u_2v_2+\ldots+u_nv_n$$
%\end{definition}
\end{expandable}


\subsection{E}

eigenspace (\ref{def:eigspace})
\begin{expandable}
    The set of all eigenvectors associated with a given eigenvalue of a matrix is known as the \dfn{eigenspace} associated with that eigenvalue.
\end{expandable}

eigenvalue (\ref{def:eigen})
\begin{expandable}
    Let $A$ be an $n \times n$ matrix.  We say that a non-zero vector $\vec{x}$ is an \dfn{eigenvector} of $A$ if $$A\vec{x} = \lambda \vec{x}$$
for some scalar $\lambda$.
We say that $\lambda$ is an \dfn{eigenvalue} of $A$ associated with the eigenvector $\vec{x}$.
\end{expandable}

eigenvalue decomposition (\ref{def:eigdecomposition})
\begin{expandable}
    If we are able to diagonalize $A$, say $A=PDP^{-1}$, we say that $PDP^{-1}$ is an \dfn{eigenvalue decomposition} of $A$.
\end{expandable}

eigenvector (\ref{def:eigen})
\begin{expandable}
    Let $A$ be an $n \times n$ matrix.  We say that a non-zero vector $\vec{x}$ is an \dfn{eigenvector} of $A$ if $$A\vec{x} = \lambda \vec{x}$$
for some scalar $\lambda$.
We say that $\lambda$ is an \dfn{eigenvalue} of $A$ associated with the eigenvector $\vec{x}$.
\end{expandable}

elementary matrix (\ref{def:elemmatrix})
\begin{expandable}
    An \dfn{elementary matrix} is a square matrix formed by applying a single elementary row operation to the identity matrix.
\end{expandable}

elementary row operations (\ref{def:elemrowops})
\begin{expandable}
    The following three operations performed on a linear system are called \dfn{elementary row operations}.
\begin{enumerate}
\item Switching the order of equations (rows) $i$ and $j$:
$$R_i\leftrightarrow R_j$$
\item Multiplying both sides of equation (row) $i$ by the same non-zero constant, $k$, and replacing equation $i$ with the result:
$$kR_i\rightarrow R_i$$
\item Adding $k$ times equation (row) $i$ to equation (row) $j$, and replacing equation $j$ with the result:
$$R_j+kR_i\rightarrow R_j$$
\end{enumerate}
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/EIG-0040/main}{equivalence relation}

equivalent linear systems (\ref{def:equivsystems})
\begin{expandable}
    Two systems of linear equations are said to be equivalent if they have the same solution set.
\end{expandable}

%Euclidean norm


\subsection{F}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0020/main}{free variable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VSP-0040/main}{fundamental subspaces of a matrix}


\subsection{G}

Gaussian elimination (\ref{def:GaussianElimination})
\begin{expandable}
    The process of using the elementary row operations on a matrix to transform it into row-echelon form is called \dfn{Gaussian Elimination}.
\end{expandable}

Gauss-Jordan elimination (\ref{def:GaussJordanElimination})
\begin{expandable}
    The process of using the elementary row operations on a matrix to transform it into reduced row-echelon form is called \dfn{Gauss-Jordan elimination}.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0040/main}{Gauss-Seidel method}

geometric multiplicity of an eigenvalue (\ref{def:geommulteig})
\begin{expandable}
    The \dfn{geometric multiplicity} of an eigenvalue $\lambda$ is the dimension of the corresponding eigenspace $\mathcal{S}_\lambda$.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/EIG-0080/main}{Gerschgorin's Disk Theorem}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/RTH-0015/main}{Gram-Schmidt Process}


\subsection{H}
% Head - Tail Formula (\ref{form:headminustailrn})
% \begin{expandable}
% %    \begin{formula}
%   [``Head - Tail'' Formula in $\RR^n$]
% Suppose a vector's tail is at point $A(a_1, a_2, \ldots ,a_n)$ and the vector's head is at $B(b_1, b_2, \ldots ,b_n)$, then 
% $$\overrightarrow{AB}=\begin{bmatrix}b_1-a_1\\ b_2-a_2\\ \vdots \\b_n-a_n\end{bmatrix}$$

% %\end{formula}
% \end{expandable}

homogeneous system (\ref{def:homogeneous})
\begin{expandable}
    A system of linear equations is called \dfn{homogeneous} if the system can be written in the form
$$\begin{array}{ccccccccc}
      a_{11}x_1 &+ &a_{12}x_2&+&\ldots&+&a_{1n}x_n&= &0 \\
	 a_{21}x_1 &+ &a_{22}x_2&+&\ldots&+&a_{2n}x_n&= &0 \\
     &&&&\vdots&&&& \\
     a_{m1}x_1 &+ &a_{m2}x_2&+&\ldots&+&a_{mn}x_n&= &0
    \end{array}$$
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/RRN-0030/main}{hyperplane}

\subsection{I}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0020/main}{identity matrix}

identity transformation (\ref{def:idtransonrn})
\begin{expandable}
    The \dfn{identity transformation} on $V$, denoted by $\id_V$, is a transformation that maps each element of $V$ to itself.

In other words,
$$\id_V:V\rightarrow V$$ is a transformation such that $$\id_V(\vec{v})=\vec{v}\quad\text{for all}\quad \vec{v} \in V$$
\end{expandable}

image of a linear transformation (\ref{def:imageofT})
\begin{expandable}
    Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be a linear transformation.  The \dfn{image} of $T$, denoted by $\mbox{im}(T)$, is the set
$$\mbox{im}(T)=\{T(\vec{v}):\vec{v}\in V\}$$
In other words, the image of $T$ consists of individual images of all vectors of $V$.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0010/main}{inconsistent system}

inner product (\ref{def:innerproductspace})
\begin{expandable}
    An \dfn{inner product} on a real vector space $V$ is a function that assigns a real number $\langle\vec{v}, \vec{w}\rangle$ to every pair $\vec{v}$, $\vec{w}$ of vectors in $V$ in such a way that the following properties are satisfied.

\begin{enumerate}
\item  $\langle\vec{v}, \vec{w}\rangle$ \textit{is a real number for all} $\vec{v}$ \textit{and} $\vec{w}$ \textit{in} $V$.

\item  $\langle\vec{v}, \vec{w}\rangle = \langle\vec{w}, \vec{v}\rangle$ \textit{for all} $\vec{v}$ \textit{and} $\vec{w}$ \textit{in} $V$.

\item  $\langle\vec{v} + \vec{w}, \vec{u}\rangle = \langle\vec{v}, \vec{u}\rangle + \langle\vec{w}, \vec{u}\rangle$ \textit{for all} $\vec{u}$, $\vec{v}$, \textit{and} $\vec{w}$ \textit{in} $V$.

\item $\langle r\vec{v}, \vec{w}\rangle = r\langle\vec{v}, \vec{w}\rangle$ \textit{for all} $\vec{v}$ \textit{and} $\vec{w}$ \textit{in} $V$ \textit{and all} $r$ \textit{in} $\RR$.

\item  $\langle\vec{v}, \vec{v}\rangle > 0$ \textit{for all} $\vec{v} \neq \vec{0}$ \textit{in} $V$.

\end{enumerate}
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VSP-0070/main}{inner product space}

inverse of a linear transformation (\ref{def:inverseoflintrans})
\begin{expandable}
    Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be a linear transformation.  A transformation $S:W\rightarrow V$ that satisfies $S\circ T=\id_V$ and $T\circ S=\id_W$ is called an \dfn{inverse} of $T$. If $T$ has an inverse, $T$ is called \dfn{invertible}.
\end{expandable}

inverse of a square matrix (\ref{def:matinverse})
\begin{expandable}
    Let $A$ be an $n\times n$ matrix.  An $n\times n$ matrix $B$ is called an \dfn{inverse} of $A$ if 
$$AB=BA=I$$
where $I$ is an $n\times n$ identity matrix.  If such an inverse matrix exists, we say that $A$ is \dfn{invertible}.  If an inverse does not exist, we say that $A$ is not invertible.
\end{expandable}

isomorphism (\ref{def:isomorphism})
\begin{expandable}
    Let $V$ and $W$ be vector spaces.  If there exists an invertible linear transformation $T:V\rightarrow W$ we say that $V$ and $W$ are \dfn{isomorphic} and write $V\cong W$.  The invertible linear transformation $T$ is called an \dfn{isomorphism}.
\end{expandable}

%iterate

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0040/main}{iterative methods}

\subsection{J}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0040/main}{Jacobi's method}


\subsection{K}

kernel of a linear transformation (\ref{def:kernel})
\begin{expandable}
    Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be a linear transformation.  The \dfn{kernel} of $T$, denoted by $\mbox{ker}(T)$, is the set
$$\mbox{ker}(T)=\{\vec{v}:T(\vec{v})=\vec{0}\}$$
In other words, the kernel of $T$ consists of all vectors of $V$ that map to $\vec{0}$ in $W$.
\end{expandable}

\subsection{L}

Laplace Expansion Theorem (\ref{th:laplace1})

leading entry (leading 1) (\ref{def:leadentry})
\begin{expandable}
    The first non-zero entry in a row of a matrix (when read from left to right) is called the \dfn{leading entry}.  When the leading entry is 1, we refer to it as a \dfn{leading 1}.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0020/main}{leading variable}

% least squares error %wait and see

% least squares solution %wait and see

linear combination of vectors (\ref{def:lincomb})
\begin{expandable}
    A vector $\vec{v}$ is said to be a \dfn{linear combination} of vectors $\vec{v}_1, \vec{v}_2,\ldots, \vec{v}_n$ if 
$$\vec{v}=a_1\vec{v}_1+ a_2\vec{v}_2+\ldots + a_n\vec{v}_n$$
for some scalars $a_1, a_2, \ldots ,a_n$.
\end{expandable}

%linear dependence/independence of matrices

linear equation (\ref{def:lineq})
\begin{expandable}
    A \dfn{linear equation} in variables $x_1, \ldots, x_n$ is an equation that can be written in the form
$$a_1x_1+a_2x_2+\ldots +a_nx_n=b$$
where $a_1,\ldots ,a_n$ and $b$ are constants.
\end{expandable}

linear transformation (\ref{def:lin}) (also see \href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/LTR-0022/main}{Linear Transformations of Abstract Vector Spaces})
\begin{expandable}
    A transformation $T:\RR^n\rightarrow \RR^m$ is called a \dfn{ linear transformation} if the following are true for all vectors $\vec{u}$ and $\vec{v}$ in $\RR^n$, and scalars $k$.
\begin{equation}
T(k\vec{u})= kT(\vec{u})
\end{equation}
\begin{equation}
T(\vec{u}+\vec{v})= T(\vec{u})+T(\vec{v})
\end{equation}
\end{expandable}

linearly dependent vectors (\ref{def:linearindependence})
\begin{expandable}
    Let $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k$ be vectors of $\RR^n$.  We say that the set $\{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k\}$ is \dfn{linearly independent} if the only solution to 
\begin{equation}c_1\vec{v}_1+c_2\vec{v}_2+\ldots +c_p\vec{v}_k=\vec{0}\end{equation}
is the \dfn{trivial solution} $c_1=c_2=\ldots =c_k=0$.

If, in addition to the trivial solution, a \dfn{non-trivial solution} (not all $c_1, c_2,\ldots ,c_k$ are zero) exists, then we say that the set $\{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k\}$ is \dfn{linearly dependent}.
\end{expandable}

linearly independent vectors (\ref{def:linearindependence})
\begin{expandable}
    Let $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k$ be vectors of $\RR^n$.  We say that the set $\{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k\}$ is \dfn{linearly independent} if the only solution to 
\begin{equation}c_1\vec{v}_1+c_2\vec{v}_2+\ldots +c_p\vec{v}_k=\vec{0}\end{equation}
is the \dfn{trivial solution} $c_1=c_2=\ldots =c_k=0$.

If, in addition to the trivial solution, a \dfn{non-trivial solution} (not all $c_1, c_2,\ldots ,c_k$ are zero) exists, then we say that the set $\{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_k\}$ is \dfn{linearly dependent}.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0070/main}{lower triangular matrix}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0070/main}{LU factorization}

\subsection{M}
Magnitude of a vector (\ref{def:normrn})
\begin{expandable}
  %  \begin{definition}
Let $\vec{v}=\begin{bmatrix}v_1\\ v_2\\ \vdots \\v_n\end{bmatrix}$ be a vector in $\RR^n$, then the \dfn{length}, or the \dfn{magnitude}, of $\vec{v}$ is given by
$$  \norm{\vec{v}}=\sqrt{v_1^2+v_2^2+\ldots +v_n^2}$$
%\end{definition}
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0010/main}{main diagonal}
    
\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0010/main}{matrix}

matrix addition (\ref{def:additionofmatrices})
\begin{expandable}
    Let $A=\begin{bmatrix} a_{ij}\end{bmatrix} $ and $B=\begin{bmatrix} b_{ij}\end{bmatrix}$ be two
$m\times n$ matrices. Then the \dfn{sum of matrices} $A$ and $B$, denoted by $A+B$,  is an $m \times n$
matrix  given by 
$$A+B=\begin{bmatrix}a_{ij}+b_{ij}\end{bmatrix}$$
\end{expandable}

matrix equality (\ref{def:equalityofmatrices})
\begin{expandable}
    Let $A=\begin{bmatrix} a_{ij}\end{bmatrix}$ and $B=\begin{bmatrix} b_{ij}\end{bmatrix}$ be two $m \times n$ matrices. Then $A=B$ means
that $a_{ij}=b_{ij}$ for all $1\leq i\leq m$ and 
$1\leq j\leq n$.
\end{expandable}

%matrix factorization

matrix multiplication (by a matrix) (\ref{def:matmatproduct})
\begin{expandable}
    Let $A$ be an $m\times n$ matrix whose rows are vectors $\vec{r}_1$, $\vec{r}_2,\ldots ,\vec{r}_n$.  Let $B$ be an $n\times p$ matrix with columns $\vec{b}_1, \vec{b}_2, \ldots, \vec{b}_p$.  Then the entries of the matrix product $AB$ are given by the dot products
$$AB=\begin{bmatrix}-&\vec{r}_1&-\\-&\vec{r}_2&-\\ &\vdots & \\-&\vec{r}_i &-\\ &\vdots& \\-&\vec{r}_m&-\end{bmatrix}\begin{bmatrix}|&|&&|&&|\\\vec{b}_1& \vec{b}_2 &\ldots  & \vec{b}_j&\ldots& \vec{b}_p\\|&|&&|&&|\end{bmatrix}=$$
$$=\begin{bmatrix}\vec{r}_1\dotp \vec{b}_1&\vec{r}_1\dotp \vec{b}_2&\ldots&\vec{r}_1\dotp \vec{b}_j&\ldots &\vec{r}_1\dotp \vec{b}_p\\\vec{r}_2\dotp \vec{b}_1&\vec{r}_2\dotp \vec{b}_2&\ldots&\vec{r}_2\dotp \vec{b}_j&\ldots &\vec{r}_2\dotp \vec{b}_p\\\vdots&\vdots&&\vdots&&\vdots\\\vec{r}_i\dotp \vec{b}_1&\vec{r}_i\dotp \vec{b}_2&\ldots&\vec{r}_i\dotp \vec{b}_j&\ldots &\vec{r}_i\dotp \vec{b}_p\\\vdots&\vdots&&\vdots&&\vdots\\\vec{r}_m\dotp \vec{b}_1&\vec{r}_m\dotp \vec{b}_2&\ldots&\vec{r}_m\dotp \vec{b}_j&\ldots &\vec{r}_m\dotp \vec{b}_p\end{bmatrix}
$$
\end{expandable}

matrix multiplication (by a scalar) (\ref{def:scalarmultofmatrices})
\begin{expandable}
    If $A=\begin{bmatrix} a_{ij}\end{bmatrix} $ and $k$ is a scalar,
then $kA=\begin{bmatrix} ka_{ij}\end{bmatrix}$. 
\end{expandable}

matrix multiplication (by a vector) (\ref{def:matrixvectormult})
\begin{expandable}
    Let $A$ be an $m\times n$ matrix, and let $\vec{x}$ be an $n\times 1$ vector.  The product $A\vec{x}$ is the $m\times 1$ vector given by:
$$A\vec{x}=\begin{bmatrix}
           a_{11} & a_{12}&\dots&a_{1n}\\
           a_{21}&a_{22} &\dots &a_{2n}\\
		\vdots & \vdots&\ddots &\vdots\\
		a_{m1}&\dots &\dots &a_{mn}
         \end{bmatrix}\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix}=
         x_1\begin{bmatrix}a_{11}\\a_{21}\\ \vdots \\a_{m1}\end{bmatrix}+
         x_2\begin{bmatrix}a_{12}\\a_{22}\\ \vdots \\a_{m2}\end{bmatrix}+\dots+
         x_n\begin{bmatrix}a_{1n}\\a_{2n}\\ \vdots \\a_{mn}\end{bmatrix}$$
or, equivalently,
$$A\vec{x}=\begin{bmatrix}a_{11}x_1+a_{12}x_2+\ldots +a_{1n}x_n\\a_{21}x_1+a_{22}x_2+\ldots +a_{2n}x_n\\\vdots\\a_{m1}x_1+a_{m2}x_2+\ldots +a_{mn}x_n\end{bmatrix}$$
\end{expandable}

matrix of a linear transformation with respect to the given bases (\ref{def:matlintransgenera})
\begin{expandable}
    Matrix $A$ of Theorem \ref{th:matlintransgeneral} is called the matrix of $T$ with respect to ordered bases $\mathcal{B}$ and $\mathcal{C}$.
\end{expandable}

%matrix powers

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/DET-0010/main}{minor of a square matrix}

\subsection{N}

%negative of a matrix

norm (\ref{def:030438})


nonsingular matrix (\ref{def:nonsingularmatrix})
\begin{expandable}
    A square matrix $A$ is said to be \dfn{nonsingular} provided that $\mbox{rref}(A)=I$.  Otherwise we say that $A$ is \dfn{singular}.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/RRN-0030/main}{normal vector}

null space of a matrix (\ref{def:nullspace})
\begin{expandable}
    Let $A$ be an $m\times n$ matrix.  The \dfn{null space} of $A$, denoted by $\mbox{null}(A)$, is the set of all vectors $\vec{x}$ in $\RR^n$ such that $A\vec{x}=\vec{0}$.
\end{expandable}

nullity of a linear transformation (\ref{def:nullityT})
\begin{expandable}
    The \dfn{nullity} of a linear transformation $T:V\rightarrow W$, is the dimension of the kernel of $T$.
$$\mbox{nullity}(T)=\mbox{dim}(\mbox{ker}(T))$$
\end{expandable}

nullity of a matrix (\ref{def:matrixnullity})
\begin{expandable}
    Let $A$ be a matrix.  The dimension of the null space of $A$ is called the \dfn{nullity} of $A$.
$$\mbox{dim}\Big(\mbox{null}(A)\Big)=\mbox{nullity}(A)$$
\end{expandable}


\subsection{O}
one-to-one (\ref{def:onetoone})
\begin{expandable}
    A linear transformation $T:V\rightarrow W$ is \dfn{one-to-one} if 
$$T(\vec{v}_1)=T(\vec{v}_2)\quad \text{implies that}\quad \vec{v}_1=\vec{v}_2$$
\end{expandable}

onto (\ref{def:onto})
\begin{expandable}
    A linear transformation $T:V\rightarrow W$ is \dfn{onto} if for every element $\vec{w}$ of $W$, there exists an element $\vec{v}$ of $V$ such that $T(\vec{v})=\vec{w}$.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VSP-0060/main}{ordered basis}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/RTH-0010/main}{orthogonal basis}

orthogonal complement of a subspace of $\RR^n$ (\ref{def:023776})
\begin{expandable}
    If $W$ is a subspace of $\RR^n$, define the \dfn{orthogonal complement} $W^\perp$ of $W$ (pronounced ``$W$-perp'') by
\begin{equation*}
W^\perp = \{\vec{x} \in\RR^n \mid \vec{x} \dotp \vec{y} = 0 \mbox{ for all } \vec{y} \in W\}
\end{equation*}
\end{expandable}

Orthogonal Decomposition Theorem (\ref{th:OrthoDecomp})
\begin{expandable}
    Let $W$ be a subspace of $\RR^n$ and let $\vec{x} \in \RR^n$.  Then there exist unique vectors $\vec{w} \in W$ and $\vec{w}^\perp \in W^\perp$ such that $\vec{x} = \vec{w} + \vec{w}^\perp$.
\end{expandable}

orthogonal matrix (\ref{def:orthogonal matrices})
\begin{expandable}
    An $n \times n$ matrix $Q$ is called an \dfn{orthogonal matrix} if it satisfies one (and hence all) of the conditions in Theorem~\ref{th:orthogonal_matrices}.
\end{expandable}

Orthogonal projection onto a subspace of $\RR^n$ (\ref{def:projOntoSubspace})
\begin{expandable}
    Let $W$ be a subspace of $\RR^n$ with orthogonal basis $\{\vec{f}_{1}, \vec{f}_{2}, \dots, \vec{f}_{m}\}$. If $\vec{x}$ is in $\RR^n$, the vector
\begin{equation}
\vec{w}=\mbox{proj}_W(\vec{x}) = \mbox{proj}_{\vec{f}_1}\vec{x} + \mbox{proj}_{\vec{f}_2}\vec{x} + \dots + \mbox{proj}_{\vec{f}_m}\vec{x}
\end{equation}
is called the \dfn{orthogonal projection} of $\vec{x}$ onto $W$. 
\end{expandable}

Orthogonal projection onto a vector (\ref{def:projection})
\begin{expandable}
    Let $\vec{v}$ be a vector, and let $\vec{d}$ be a non-zero vector.  The \dfn{projection of $\vec{v}$ onto $\vec{d}$} is given by 
$$\mbox{proj}_{\vec{d}}\vec{v}=\left(\frac{\vec{v}\dotp\vec{d}}{\norm{\vec{d}}^2}\right)\vec{d}$$
\end{expandable}

orthogonal set of vectors (\ref{orthset})
\begin{expandable}
    Let $\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ be a set of nonzero
vectors in $\RR^n$. Then this set is called an
\dfn{orthogonal set} if 
$\vec{v}_i \dotp \vec{v}_j = 0$ for all $i \neq j$.
Moreover, if $\norm{\vec{v}_i}=1$ for $i=1,\ldots,m$ (i.e. each vector in the set is a unit vector), we say the set of vectors is an \dfn{orthonormal set}.
\end{expandable}

Orthogonal vectors (\ref{def:orthovectors}) 
\begin{expandable}
 %   \begin{definition}
Let $\vec{u}$ and $\vec{v}$ be vectors in $\RR^n$. We say $\vec{u}$ and $\vec{v}$ are \dfn{orthogonal} if $\vec{u}\dotp \vec{v}=0$.
%\end{definition}
\end{expandable}


orthogonally diagonalizable matrix (\ref{def:orthDiag})
\begin{expandable}
    An $n \times n$ matrix $A$ is said to be \dfn{orthogonally diagonalizable} if an orthogonal matrix $Q$ can be found such that  $Q^{-1}AQ = Q^{T}AQ$ is diagonal.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/RTH-0010/main}{orthonormal basis}

orthonormal set of vectors (\ref{orthset})
\begin{expandable}
    Let $\{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_k \}$ be a set of nonzero
vectors in $\RR^n$. Then this set is called an
\dfn{orthogonal set} if 
$\vec{v}_i \dotp \vec{v}_j = 0$ for all $i \neq j$.
Moreover, if $\norm{\vec{v}_i}=1$ for $i=1,\ldots,m$ (i.e. each vector in the set is a unit vector), we say the set of vectors is an \dfn{orthonormal set}.
\end{expandable}

%outer product



\subsection{P}



parametric equation of a line (\ref{form:paramlinend})
\begin{expandable}
    Let $\vec{v}=\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}$ be a direction vector for line $l$ in $\RR^n$, and let $(a_1, a_2,\ldots , a_n)$ be an arbitrary point on $l$.  Then the following parametric equations describe $l$:
\[
x_1=v_1t+a_1\]
\[x_2=v_2t+a_2\]
\[\vdots\]
\[x_n=v_nt+a_n
\]
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0050/main}{particular solution}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0023/main}{partitioned matrices (block multiplication)}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0070/main}{permutation matrix}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0030/main}{pivot}

positive definite matrix (\ref{def:024811})
\begin{expandable}
    A square matrix is called \dfn{positive definite} if it is symmetric and all its eigenvalues $\lambda$ are positive.  We write $\lambda>0$ when eigenvalues are real and positive.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/EIG-0070/main}{power method (and its variants)}

% properties of determinants

% properties of matrix algebra 

% properties of orthogonal matrices

\subsection{Q}

QR factorization (\ref{def:QR-factorization})
\begin{expandable}
    Let $A$ be an $m \times n$ matrix with independent columns. A \dfn{QR-factorization} of $A$ expresses it as $A = QR$ where $Q$ is $m \times n$ with orthonormal columns and $R$ is an invertible and upper triangular matrix with positive diagonal entries.
\end{expandable}

\subsection{R}

%range of a linear transformation

rank of a linear transformation (\ref{def:rankofT})
\begin{expandable}
    The \dfn{rank} of a linear transformation $T:V\rightarrow W$, is the dimension of the image of $T$.
$$\mbox{rank}(T)=\mbox{dim}(\mbox{im}(T))$$
\end{expandable}

rank of a matrix (\ref{def:rankofamatrix}) (\ref{th:dimofrowA})
\begin{expandable}
    The \dfn{rank} of matrix $A$, denoted by $\mbox{rank}(A)$, is the number of nonzero rows that remain after we reduce $A$ to row-echelon form by elementary row operations.

    For any matrix $A$,  
$$\mbox{rank}(A)=\mbox{dim}\Big(\mbox{row}(A)\Big)$$

\end{expandable}

Rank-Nullity Theorem for linear transformations (\ref{th:ranknullityforT})
\begin{expandable}
    Let $T:V\rightarrow W$ be a linear transformation.  Suppose $\mbox{dim}(V)=n$, then
$$\mbox{rank}(T)+\mbox{nullity}(T)=n$$
\end{expandable}

Rank-Nullity Theorem for matrices (\ref{th:matrixranknullity})
\begin{expandable}
    Let $A$ be an $m\times n$ matrix.  Then 
$$\mbox{rank}(A)+\mbox{nullity}(A)=n$$
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/EIG-0070/main}{Rayleigh quotients}

reduced row echelon form (\ref{def:rref})
\begin{expandable}
    A matrix that is already in \dfn{row-echelon} form is said to be in \dfn{reduced row-echelon form} if:
\begin{enumerate}
\item Each leading entry is $1$
\item All entries {\it above} and below each leading $1$ are $0$
\end{enumerate}
\end{expandable}

redundant vectors (\ref{def:redundant})
\begin{expandable}
    Let $\{\vec{v}_1,\vec{v}_2,\dots,\vec{v}_k\}$ be a set of vectors in $\RR^n$.  If we can remove one vector without changing the span of this set, then that vector is \dfn{redundant}.  In other words, if $$\mbox{span}\left(\vec{v}_1,\vec{v}_2,\dots,\vec{v}_k\right)=\mbox{span}\left(\vec{v}_1,\vec{v}_2,\dots,\vec{v}_{j-1},\vec{v}_{j+1},\dots,\vec{v}_k\right)$$ we say that $\vec{v}_j$ is a redundant element of $\{\vec{v}_1,\vec{v}_2,\dots,\vec{v}_k\}$, or simply redundant.
\end{expandable}

%representation of matrix products

row echelon form (\ref{def:ref})
\begin{expandable}
    A matrix is said to be in \dfn{row-echelon form} if:
\begin{enumerate}
\item All entries below each leading entry are 0.
\item Each leading entry is in a column to the right of the leading entries in the rows above it.
\item All rows of zeros, if there are any, are located below non-zero rows.
\end{enumerate}
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0020/main}{row equivalent matrices}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0010/main}{row matrix (vector)}

row space of a matrix (\ref{def:rowspace})
\begin{expandable}
    Let $A$ be an $m\times n$ matrix.  The \dfn{row space} of $A$, denoted by $\mbox{row}(A)$, is the subspace of $\RR^n$ spanned by the rows of $A$.
\end{expandable}


\subsection{S}
\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VEC-0010/main}{Scalar} 

Standard Unit Vectors (\ref{def:standardunitvecrn})
\begin{expandable}
    %\begin{definition}
  Let $\vec{e}_i$ denote a vector that has $1$ as the $i^{th}$ component and zeros elsewhere.  In other words, $$\vec{e}_i=\begin{bmatrix}
0\\
0\\
\vdots\\
1\\
\vdots\\
0
\end{bmatrix}$$ 
  where $1$ is in the $i^{th}$ position.  We say that  $\vec{e}_i$ is a \dfn{standard unit vector of $\RR^n$}.
%\end{definition}
\end{expandable}

%scalar multiple of a matrix

similar matrices (\ref{def:similar})
\begin{expandable}
    If $A$ and $B$ are $n \times n$ matrices, we say that $A$ and $B$ are \dfn{similar}, if $B = P^{-1}AP$ for some invertible matrix $P$.  In this case we write $A \sim B$.
\end{expandable}

singular matrix (\ref{def:nonsingularmatrix})
\begin{expandable}
    A square matrix $A$ is said to be \dfn{nonsingular} provided that $\mbox{rref}(A)=I$.  Otherwise we say that $A$ is \dfn{singular}.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/RTH-0060/main}{singular value decomposition (SVD)}

singular values (\ref{singularvalues})
\begin{expandable}
    Let $A$ be an $m\times n$ matrix. The \dfn{singular values} of $A$ are the square roots of the positive
eigenvalues of $A^TA.$ 
\end{expandable}

skew symmetric matrix (\ref{def:symmetricandskewsymmetric})
\begin{expandable}
    An $n\times n$ matrix $A$ is said to be
\dfn{symmetric} if $A=A^{T}.$ It is said to be
\dfn{skew symmetric} if $A=-A^{T}.$
\end{expandable}

%span of a set of matrices

span of a set of vectors (\ref{def:span})
\begin{expandable}
    Let $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$ be vectors in $\RR^n$.  The set $S$ of all linear combinations of $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$ is called the \dfn{span} of $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$.  We write 
$$S=\mbox{span}(\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p)$$
and we say that vectors $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$ \dfn{span} $S$.  Any vector in $S$ is said to be \dfn{in the span} of $\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p$.  The set $\{\vec{v}_1, \vec{v}_2,\ldots ,\vec{v}_p\}$ is called a \dfn{spanning set} for $S$.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VEC-0090/main}{spanning set}

%spectral decomposition

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/RTH-0035/main}{Spectral Theorem}

standard basis (\ref{def:standardbasis})
\begin{expandable}
    The set $\{\vec{e}_1, \ldots ,\vec{e}_n\}$ is called the \dfn{standard basis} of $\RR^n$.
\end{expandable}

strictly diagonally dominant (\ref{def:strict_diag_dom})
\begin{expandable}
    Let $A=[a_{ij}]$ be the $n\times n$ matrix which is the coefficient matrix of the linear system $A \vec{x}= \vec{b}$.  Let
$$
r_i(A):= \sum_{j \ne i} |a_{ij}|
$$
denote the sum of the absolute values of the non-diagonal entries in row $i$.  We say that $A$ is \dfn{strictly diagonally dominant} if 
$$|a_{ii}|>r_i(A)$$
for all values of $i$ from $i=1$ to $i=n$.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0010/main}{square matrix}

standard matrix of a linear transformation (\ref{def:standardmatoflintrans})
\begin{expandable}
    The matrix in Theorem \ref{th:matlin} is known as the \dfn{standard matrix of the linear transformation} $T$.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VEC-0010/main}{Standard Position}

subspace of $\RR^n$ (\ref{def:subspace})
\begin{expandable}
    Suppose that $V$ is a nonempty subset of $\RR^n$ that is closed under addition and closed under scalar multiplication.  Then $V$ is a \dfn{subspace} of $\RR^n$.
\end{expandable}

subspace (\ref{def:subspaceabstract})
\begin{expandable}
    A nonempty subset $U$ of a vector space $V$ is called a \dfn{subspace} of $V$, provided that $U$ is itself a vector space when given the same addition and scalar multiplication as $V$.
\end{expandable}

subspace test (\ref{th:subspacetestabstract})
\begin{expandable}
    Let $U$ be a nonempty subset of a vector space $V$.  If $U$ is closed under the operations of addition and scalar multiplication of $V$, then $U$ is a subspace of $V$.
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VEC-0030/main}{Subtraction of vectors}

symmetric matrix (\ref{def:symmetricandskewsymmetric})
\begin{expandable}
    An $n\times n$ matrix $A$ is said to be
\dfn{symmetric} if $A=A^{T}.$ It is said to be
\dfn{skew symmetric} if $A=-A^{T}.$
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/SYS-0010/main}{system of linear equations}



\subsection{T}

trace of a matrix (\ref{def:trace})
\begin{expandable}
    The \dfn{trace} of an $n \times n$ matrix $A$, abbreviated $\mbox{tr} A$, is defined to be the sum of the main diagonal elements of $A$.  In other words, if $ A = [a_{ij}]$, then $$\mbox{tr}(A) = a_{11} + a_{22} + \dots + a_{nn}$$  We may also write $\mbox{tr}(A) =\sum_{i=1}^n a_{ii}$.
\end{expandable}

transpose of a matrix (\ref{def:matrixtranspose})
\begin{expandable}
    Let $A=\begin{bmatrix} a _{ij}\end{bmatrix}$ be an $m\times n$ matrix. Then the \dfn{transpose of $A$}, denoted by $A^{T}$, is the $n\times m$
matrix given by 
\begin{equation*}
A^{T} = \begin{bmatrix} a _{ij}\end{bmatrix}^{T}= \begin{bmatrix} a_{ji} \end{bmatrix}
\end{equation*}
\end{expandable}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/APX-0010/main}{triangle inequality}

\subsection{U}
\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VEC-0036/main}{Unit Vector}

\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/MAT-0070/main}{upper triangular matrix}

\subsection{V}
\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VEC-0010/main}{Vector}

Vector equation of a line (\ref{form:vectorlinend})
\begin{expandable}
    Let $\vec{v}=\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}$ be a direction vector for line $l$ in $\RR^n$, and let $(a_1, a_2,\ldots , a_n)$ be an arbitrary point on $l$.  Then the following vector equation describes $l$:
$$\vec{x}(t)=\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}t+\begin{bmatrix}a_1\\a_2\\\vdots\\a_n\end{bmatrix}$$
\end{expandable}

vector space (\href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VSP-0020/main}{$\RR^n$})
(\ref{def:vectorspacegeneral})
\begin{expandable}
    Let $V$ be a nonempty set.  Suppose that elements of $V$ can be added together and multiplied by scalars.  The set $V$, together with operations of addition and scalar multiplication, is called a \dfn{vector space} provided that 
  \begin{itemize}
  \item[] $V$ is closed under addition
  \item[] $V$ is closed under scalar multiplication
  \end{itemize}
  and the following properties hold for $\vec{u}$, $\vec{v}$ and $\vec{w}$ in $V$ and scalars $k$ and $p$:
  \begin{enumerate}
   \item 
  Commutative Property of Addition:\quad
  $\vec{u}+\vec{v}=\vec{v}+\vec{u}$
  \item 
  Associative Property of Addition:\quad
  $(\vec{u}+\vec{v})+\vec{w}=\vec{u}+(\vec{v}+\vec{w})$
  \item 
  Existence of Additive Identity:\quad
  $\vec{u}+\vec{0}=\vec{u}$
  \item 
  Existence of Additive Inverse:\quad
  $\vec{u}+(-\vec{u})=\vec{0}$
  \item 
  Distributive Property over Vector Addition:\quad
  $k(\vec{u}+\vec{v})=k\vec{u}+k\vec{v}$
  \item 
  Distributive Property over Scalar Addition:\quad
  $(k+p)\vec{u}=k\vec{u}+p\vec{u}$
  \item 
  Associative Property for Scalar Multiplication:\quad
  $k(p\vec{u})=(kp)\vec{u}$
  \item 
  Multiplication by $1$:\quad
  $1\vec{u}=\vec{u}$
  \end{enumerate}
We will refer to elements of $V$ as \dfn{vectors}.  
\end{expandable}

\subsection{W}

\subsection{X}

\subsection{Y}

\subsection{Z}

zero matrix (\ref{def:zeromatrix})
\begin{expandable}
    The $m\times n$ \dfn{zero matrix} is the $m\times n$ matrix
having every entry equal to zero. The zero matrix is
denoted by $O$.
\end{expandable}

zero transformation (\ref{def:zerotransonrn})
\begin{expandable}
    The \dfn{zero transformation}, $Z$, maps every element of the domain to the zero vector.

In other words,
$$Z:V\rightarrow W$$ is a transformation such that $$Z(\vec{v})=\vec{0}\quad\text{for all}\quad \vec{v} \in V$$
\end{expandable}

\end{document}

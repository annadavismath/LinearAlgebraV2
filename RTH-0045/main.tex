\documentclass{ximera}
\input{../preamble.tex}

 \title{Positive Definite Matrices} \license{CC BY-NC-SA 4.0}

\begin{document}

\begin{abstract}

\end{abstract}
\maketitle

All the eigenvalues of any symmetric
matrix are real; this section is about the case in which the eigenvalues
 are positive. These matrices, which arise whenever optimization
(maximum and minimum) problems are encountered, have countless
applications throughout science and engineering. They also arise in
statistics (for example, in factor analysis used in the social sciences)
 and in geometry (quadratic forms, for instance). We will encounter them again in \href{}{Inner Product Spaces} when describing all inner products in $\RR^n$.


\begin{definition}\label{024811}
A square matrix is called \dfn{positive definite} if it is symmetric and all its eigenvalues $\lambda$ are positive, that is $\lambda > 0$.
\end{definition}

Because these matrices are symmetric, the Real Spectral Theorem plays a central role in the theory.


\begin{theorem}\label{thm:024815}
If $A$ is positive definite, then it is invertible and $\mbox{det}A > 0$.
\end{theorem}

\begin{proof}
If $A$ is $n \times n$ and the eigenvalues are $\lambda_{1}, \lambda_{2}, \dots, \lambda_{n}$, then $\mbox{det}A = \lambda_{1}\lambda_{2} \cdots \lambda_{n} > 0$ by the Real Spectral Theorem (or, more specifically, \ref{cor:det_and_tr}).
\end{proof}

If $\vec{x}$ is a column in $\RR^n$ and $A$ is any real $n \times n$ matrix, we view the $1 \times 1$ matrix $\vec{x}^{T}A\vec{x}$ as a real number. With this convention, we have the following characterization of positive definite matrices.


\begin{theorem}\label{thm:024830}
A symmetric matrix $A$ is positive definite if and only if $\vec{x}^{T} A \vec{x} > 0$ for every column $\vec{x} \neq \vec{0}$ in $\RR^n$.
\end{theorem}

\begin{proof}
$A$ is symmetric so, by the Real Spectral Theorem, let $P^{T}AP = D = \mbox{diag}(\lambda_{1}, \lambda_{2}, \dots, \lambda_{n})$ where $P^{-1} = P^{T}$ and the $\lambda_{i}$ are the eigenvalues of $A$. Given a column $\vec{x}$ in $\RR^n$, write $\vec{y} = P^{T}\vec{x} = \left[ \begin{array}{cccc}
y_{1} & y_{2} & \dots & y_{n}
\end{array}\right]^T$. Then
\begin{equation} \label{symmetricEq}
\vec{x}^TA\vec{x} = \vec{x}^T(PDP^T)\vec{x} = \vec{y}^TD\vec{y} = \lambda_{1}y_{1}^2 + \lambda_{2}y_{2}^2 + \dots + \lambda_{n}y_{n}^2
\end{equation}
If $A$ is positive definite and $\vec{x} \neq \vec{0}$, then $\vec{x}^{T}A\vec{x} > 0$ by (\ref{symmetricEq}) because some $y_{j} \neq 0$ and every $\lambda_{i} > 0$. Conversely, if $\vec{x}^{T}A\vec{x} > 0$ whenever $\vec{x} \neq \vec{0}$, let $\vec{x} = P\vec{e}_{j} \neq \vec{0}$ where $\vec{e}_{j}$ is column $j$ of $I_{n}$. Then $\vec{y} = \vec{e}_{j}$, so (\ref{symmetricEq}) reads $\lambda_{j} = \vec{x}^{T}A\vec{x} > 0$.
\end{proof}

Note that Theorem~\ref{thm:024830} shows that the positive definite matrices are exactly the symmetric matrices $A$ for which the quadratic form $q = \vec{x}^{T}A\vec{x}$ takes only positive values.

\begin{example}\label{exa:024865}
If $U$ is any invertible $n \times n$ matrix, show that $A = U^{T}U$ is positive definite.


\begin{solution}
  If $\vec{x}$ is in $\RR^n$ and $\vec{x} \neq \vec{0}$, then
\begin{equation*}
\vec{x}^TA\vec{x} =  \vec{x}^T(U^TU)\vec{x} = (U\vec{x})^T(U\vec{x})= \norm{ U\vec{x} }^2 > 0
\end{equation*}
because $U\vec{x} \neq \vec{0}$ ($U$ is invertible). Hence Theorem~\ref{thm:024830} applies.
\end{solution}
\end{example}

It is remarkable that the converse to Example~\ref{exa:024865} is also true. In fact every positive definite matrix $A$ can be factored as $A = U^{T}U$ where $U$ is an upper triangular matrix with positive elements on the main
diagonal. However, before verifying this, we introduce another concept
that is central to any discussion of positive definite matrices.


If $A$ is any $n \times n$ matrix, let $^{(r)}A$ denote the $r \times r$ submatrix in the upper left corner of $A$; that is, $^{(r)}A$ is the matrix obtained from $A$ by deleting the last $n - r$ rows and columns. The matrices $^{(1)}A, ^{(2)}A, ^{(3)}A, \dots$, $^{(n)}A = A$ are called the \dfn{principal submatrices} of $A$.


\begin{example}\label{exa:024883}
If $A = \left[ \begin{array}{rrr}
10 & 5 & 2 \\
5 & 3 & 2 \\
2 & 2 & 3
\end{array}\right]$ then $^{(1)}A = \left[ 10 \right]$, $^{(2)}A = \left[ \begin{array}{rr}
10 & 5 \\
5 & 3
\end{array}\right]$ and $^{(3)}A = A$.
\end{example}

\begin{lemma}\label{lem:024890}
If $A$ is positive definite, so is each principal submatrix $^{(r)}A$ for $r = 1, 2, \dots, n$.
\end{lemma}

\begin{proof}
Write $A = \left[ \begin{array}{rr}
^{(r)}A & P \\
Q & R
\end{array}\right]$
 in block form. If $\vec{y} \neq \vec{0}$ in $\RR^r$, write $\vec{x} = \left[ \begin{array}{r}
 \vec{y} \\
 \vec{0}
 \end{array}\right]$ in $\RR^n$.

Then $\vec{x} \neq \vec{0}$, so the fact that $A$ is positive definite gives
\begin{equation*}
0 < \vec{x}^TA\vec{x} = \left[ \begin{array}{rr}
\vec{y}^T & \vec{0}
\end{array}\right] \left[ \begin{array}{rr}
^{(r)}A & P \\
Q & R
\end{array}\right] \left[ \begin{array}{r}
\vec{y} \\
\vec{0}
\end{array}\right] = \vec{y}^T(^{(r)}A)\vec{y}
\end{equation*}
This shows that $^{(r)}A$ is positive definite by Theorem~\ref{thm:024830}.\footnote{A similar argument shows that, if $B$ is any matrix obtained from a positive definite matrix $A$ by deleting certain rows and deleting the \textit{same} columns, then $B$ is also positive definite.}
\end{proof}

If $A$ is positive definite, Lemma~\ref{lem:024890} and Theorem~\ref{thm:024815} show that $\mbox{det}(^{(r)}A) > 0$ for every $r$. This proves part of the following theorem which contains the converse to Example~\ref{exa:024865}, and characterizes the positive definite matrices among the symmetric ones.


\begin{theorem}\label{thm:024907}
The following conditions are equivalent for a symmetric $n \times n$ matrix $A$:

\begin{enumerate}
\item\label{thm:024907a} $A$ is positive definite.

\item\label{thm:024907b} $\mbox{det}(^{(r)}A) > 0$ for each $r = 1, 2, \dots, n$.

\item\label{thm:024907c} $A = U^{T}U$ where $U$ is an upper triangular matrix with positive entries on the main diagonal.
\end{enumerate}

Furthermore, the factorization in \ref{thm:024907c} is unique (called the \dfn{Cholesky factorization} of $A$).
\end{theorem}
\begin{remark}
    Andre-Louis
 Cholesky (1875--1918), was a French mathematician who died in World War
I. His factorization was published in 1924 by a fellow officer.
\end{remark}

\begin{proof}
First, \ref{thm:024907c} $\Rightarrow$ \ref{thm:024907a} by Example~\ref{exa:024865}, and \ref{thm:024907a} $\Rightarrow$ \ref{thm:024907b} by Lemma~\ref{lem:024890} and Theorem~\ref{thm:024815}.

\ref{thm:024907b} $\Rightarrow$ \ref{thm:024907c}. Assume \ref{thm:024907b} and proceed by induction on $n$. If $n = 1$, then $A = \left[ a \right]$ where $a > 0$ by \ref{thm:024907b}, so take $U = \left[ \sqrt{a} \right]$. If $n > 1$, write $B =^{(n-1)}A$. Then $B$ is symmetric and satisfies \ref{thm:024907b} so, by induction, we have $B = U^{T}U$ as in \ref{thm:024907c} where $U$ is of size $(n - 1) \times (n - 1)$. Then, as $A$ is symmetric, it has block form $A = \left[ \begin{array}{cc}
B & \vec{p} \\
\vec{p}^T & b
\end{array}\right]$ where $\vec{p}$ is a column in $\RR^{n-1}$ and $b$ is in $\RR$. If we write $\vec{x} = (U^{T})^{-1}\vec{p}$ and $c = b - \vec{x}^{T}\vec{x}$, block multiplication gives
\begin{equation*}
A = \left[ \begin{array}{cc}
U^TU & \vec{p} \\
\vec{p}^T & b
\end{array}\right] = \left[ \begin{array}{cc}
U^T & 0 \\
\vec{x}^T & 1
\end{array}\right] \left[ \begin{array}{cc}
U & \vec{x} \\
0 & c
\end{array}\right]
\end{equation*}
as the reader can verify. Taking determinants and applying Theorem~\ref{thm:007890} gives $\mbox{det}A = \mbox{det}(U^{T}) \mbox{det} U \cdot c = c(\mbox{det}U)^{2}$. Hence $c > 0$ because $\mbox{det}A > 0$ by \ref{thm:024907b}, so the above factorization can be written
\begin{equation*}
A = \left[ \begin{array}{cc}
U^T & 0 \\
\vec{x}^T & \sqrt{c}
\end{array}\right] \left[ \begin{array}{cc}
U & \vec{x} \\
0 & \sqrt{c}
\end{array}\right]
\end{equation*}
Since $U$ has positive diagonal entries, this proves \ref{thm:024907c}.

As to the uniqueness, suppose that $A = U^TU = U_{1}^TU_{1}$ are two Cholesky factorizations. Now write $D = UU_{1}^{-1} = (U^T)^{-1}U_{1}^T$. Then $D$ is upper triangular, because $D = UU_{1}^{-1}$, and lower triangular, because $D = (U^T)^{-1}U_{1}^T$, and so it is a diagonal matrix. Thus $U = DU_{1}$ and $U_{1} = DU$, so it suffices to show that $D = I$. But eliminating $U_{1}$ gives $U = D^{2}U$, so $D^{2} = I$ because $U$ is invertible. Since the diagonal entries of $D$ are positive (this is true of $U$ and $U_{1}$), it follows that $D = I$.
\end{proof}

The remarkable thing is that the matrix $U$ in the Cholesky factorization is easy to obtain from $A$ using row operations. The key is that Step 1 of the following algorithm is \textit{possible} for any positive definite matrix $A$. A proof of the algorithm is given following Example~\ref{exa:024959}.


\begin{theorem}[Algorithm for the Cholesky Factorization]\label{thm:024947}
If $A$ is a positive definite matrix, the Cholesky factorization $A = U^{T}U$ can be obtained as follows:


\begin{itemize}
\item[Step 1.] Carry $A$ to an upper triangular matrix $U_{1}$ with positive diagonal entries using row operations each of which adds a multiple of a row to a lower row.

\item[Step 2.] Obtain $U$ from $U_{1}$ by dividing each row of $U_{1}$ by the square root of the diagonal entry in that row.

\end{itemize}
\end{theorem}

\begin{example}\label{exa:024959}
Find the Cholesky factorization of $A = \left[ \begin{array}{rrr}
10 & 5 & 2 \\
5 & 3 & 2 \\
2 & 2 & 3
\end{array}\right]$.


\begin{solution}
  The matrix $A$ is positive definite by Theorem~\ref{thm:024907} because $\mbox{det}{^{(1)}A} = 10 > 0$, $\mbox{det}{^{(2)}A} = 5 > 0$, and $\mbox{det}{^{(3)}A} = \mbox{det} A = 3 > 0$. Hence Step 1 of the algorithm is carried out as follows:
\begin{equation*}
A = \left[ \begin{array}{rrr}
10 & 5 & 2 \\
5 & 3 & 2 \\
2 & 2 & 3
\end{array}\right] \rightarrow \left[ \begin{array}{rrc}
10 & 5 & 2 \\
0 & \frac{1}{2} & 1 \\
0 & 1 & \frac{13}{5}
\end{array}\right] \rightarrow \left[ \begin{array}{rrr}
10 & 5 & 2 \\
0 & \frac{1}{2} & 1 \\
0 & 0 & \frac{3}{5}
\end{array}\right] = U_{1}
\end{equation*}
Now carry out Step 2 on $U_{1}$ to obtain $U = \left[ \def\arraystretch{1.5}\begin{array}{ccc}
\sqrt{10} & \frac{5}{\sqrt{10}} & \frac{2}{\sqrt{10}} \\
0 & \frac{1}{\sqrt{2}} & \sqrt{2} \\
0 & 0 & \frac{\sqrt{3}}{\sqrt{5}}
\end{array}\right]$.

The reader can verify that $U^{T}U = A$.
\end{solution}
\end{example}

\begin{proof}[Proof of the Cholesky Algorithm]
If $A$ is positive definite, let $A = U^{T}U$ be the Cholesky factorization, and let $D = \mbox{diag}(d_{1}, \dots, d_{n})$ be the common diagonal of $U$ and $U^{T}$. Then $U^{T}D^{-1}$ is lower triangular with ones on the diagonal (call such matrices LT-1). Hence $L = (U^{T}D^{-1})^{-1}$ is also LT-1, and so $I_{n} \to L$ by a sequence of row operations each of which adds a multiple of a row
to a lower row (verify; modify columns right to left). But then $A \to LA$ by the same sequence of row operations. Since $LA = [D(U^{T})^{-1}][U^{T}U] = DU$ is upper triangular with positive entries on the diagonal, this shows that Step 1 of the algorithm is possible.

Turning to Step 2, let $A \to U_{1}$ as in Step 1 so that $U_{1} = L_{1}A$ where $L_{1}$ is
LT-1. Since A is symmetric, we get
\begin{equation} \label{symmetricEq2}
L_{1}U_{1}^T = L_{1}(L_{1}A)^T = L_{1}A^TL_{1}^T = L_{1}AL_{1}^T = U_{1}L_{1}^T
\end{equation}
Let $D_{1} = \mbox{diag}(e_{1}, \dots, e_{n})$ denote the diagonal of $U_{1}$. Then (\ref{symmetricEq2}) gives $L_{1}(U_{1}^TD_{1}^{-1}) = U_{1}L_{1}^TD_{1}^{-1}$. This is both upper triangular (right side) and LT-1 (left side), and so must equal $I_{n}$. In particular, $U_{1}^TD_{1}^{-1} = L_{1}^{-1}$. Now let $D_{2} = \mbox{diag}(\sqrt{e_{1}}, \dots, \sqrt{e_{n}})$, so that $D_{2}^2 = D_{1}$. If we write $U = D_{2}^{-1}U_{1}$
 we have
\begin{equation*}
U^TU = (U_{1}^TD_{2}^{-1})(D_{2}^{-1}U_{1}) = U_{1}^T(D_{2}^2)^{-1}U_{1} = (U_{1}^TD_{1}^{-1})U_{1} = (L_{1}^{-1})U_{1} = A
\end{equation*}
This proves Step 2 because $U = D_{2}^{-1}U_{1}$ is formed by dividing each row of $U_{1}$ by the square root of its diagonal entry (verify).
\end{proof}

\section*{Practice Problems}

\begin{problem}\label{prob:pos_def_1}
Find the Cholesky decomposition of each of the following matrices.

\begin{enumerate}
\item $\left[ \begin{array}{rr}
4 & 3 \\
3 & 5
\end{array}\right]$

\item $\left[ \begin{array}{rr}
2 & -1 \\
-1 & 1
\end{array}\right]$

Click the arrow to see the answer.
\begin{expandable}
$U = \frac{\sqrt{2}}{2} \left[ \begin{array}{rr}
2 & -1 \\
0 & 1
\end{array}\right]$
\end{expandable}

\item$\left[ \begin{array}{rrr}
12 & 4 & 3 \\
4 & 2 & -1 \\
3 & -1 & 7
\end{array}\right]$

\item $\left[ \begin{array}{rrr}
20 & 4 & 5 \\
4 & 2 & 3 \\
5 & 3 & 5
\end{array}\right]$

Click the arrow to see the answer.
\begin{expandable}
$U = \frac{1}{30} \left[ \begin{array}{ccc}
60\sqrt{5} & 12\sqrt{5} & 15\sqrt{5} \\
0 & 6\sqrt{30} & 10\sqrt{30} \\
0 & 0 & 5\sqrt{15}
\end{array}\right]$
\end{expandable}

\end{enumerate}

\end{problem}

\begin{problem}\label{prob:pos_def_2}
\begin{enumerate}[label={\alph*.}]
\item If $A$ is positive definite, show that $A^{k}$ is positive definite for all $k \geq 1$.

\item Prove the converse to (a) when $k$ is odd.

\begin{hint}
If $\lambda^{k} > 0$, $k$ odd, then $\lambda > 0$.
\end{hint}

\item Find a symmetric matrix $A$ such that $A^{2}$ is positive definite but $A$ is not.

\end{enumerate}

\end{problem}

\begin{problem}\label{prob:pos_def_3}
Let $A = \left[ \begin{array}{rr}
1 & a \\
a & b
\end{array}\right]$. If $a^{2} < b$, show that $A$ is positive definite and find the Cholesky factorization.
\end{problem}

\begin{problem}\label{prob:pos_def_4}
If $A$ and $B$ are positive definite and $r > 0$, show that $A + B$ and $rA$ are both positive definite.

Click the arrow to see the answer.
\begin{expandable}
If $\vec{x} \neq \vec{0}$, then $\vec{x}^{T}A\vec{x} > 0$ and $\vec{x}^{T}B\vec{x} > 0$. Hence $\vec{x}^{T}(A + B)\vec{x} = \vec{x}^{T}A\vec{x} + \vec{x}^{T}B\vec{x} > 0$ and $\vec{x}^{T}(rA)\vec{x} = r(\vec{x}^{T}A\vec{x}) > 0$, as $r > 0$.
\end{expandable}

\end{problem}

\begin{problem}\label{prob:pos_def_5}
If $A$ and $B$ are positive definite, show that $\left[ \begin{array}{rr}
A & 0 \\
0 & B
\end{array}\right]$ is positive definite.
\end{problem}

\begin{problem}\label{prob:pos_def_6}
If $A$ is an $n \times n$ positive definite matrix and $U$ is an $n \times m$ matrix of rank $m$, show that $U^{T}AU$ is positive definite.

\begin{hint}
Let $\vec{x} \neq \vec{0}$ in $\RR^n$. Then $\vec{x}^{T}(U^{T}AU)\vec{x} = (U\vec{x})^{T}A(U\vec{x}) > 0$ provided $U\vec{x} \neq 0$. But if $U = \left[ \begin{array}{cccc}
\vec{c}_{1} & \vec{c}_{2} & \dots &  \vec{c}_{n}
\end{array}\right]$ and $\vec{x} = (x_{1}, x_{2}, \dots, x_{n})$, then $U\vec{x} = x_{1}\vec{c}_{1} + x_{2}\vec{c}_{2} + \dots  + x_{n}\vec{c}_{n} \neq \vec{0}$ because $\vec{x} \neq \vec{0}$ and the $\vec{c}_{i}$ are independent.
\end{hint}
\end{problem}

\begin{problem}\label{prob:pos_def_7}
If $A$ is positive definite, show that each diagonal entry is positive.
\end{problem}

\begin{problem}\label{prob:pos_def_8}
Let $A_{0}$ be formed from $A$ by deleting rows 2 and 4 and deleting columns 2 and 4. If $A$ is positive definite, show that $A_{0}$ is positive definite.
\end{problem}

\begin{problem}\label{prob:pos_def_9}
If $A$ is positive definite, show that \\ $A = CC^{T}$ where $C$ has orthogonal columns.
\end{problem}

\begin{problem}\label{prob:pos_def_10}
If $A$ is positive definite, show that $A = C^{2}$ where $C$ is positive definite.

\begin{hint}
Let $P^{T}AP = D = \mbox{diag}(\lambda_{1}, \dots, \lambda_{n})$ where $P^{T} = P$. Since $A$ is positive definite, each eigenvalue $\lambda_{i} > 0$. If $B = \mbox{diag}(\sqrt{\lambda_{1}}, \dots, \sqrt{\lambda_{n}})$ then $B^{2} = D$, so $A = PB^{2}P^{T} = (PBP^{T})^{2}$. Take $C = PBP^{T}$. Since $C$ has eigenvalues $\sqrt{\lambda_{i}} > 0$, it is positive definite.
\end{hint}
\end{problem}

\begin{problem}\label{prob:pos_def_11}
Let $A$ be a positive definite matrix. If $a$ is a real number, show that $aA$ is positive definite if and only if $a > 0$.
\end{problem}

\begin{problem}\label{prob:pos_def_12}
\begin{enumerate}
\item Suppose an invertible matrix $A$ can be factored in $\mathbb{M}_{nn}$ as $A = LDU$ where $L$ is lower triangular with $1$s on the diagonal, $U$ is upper triangular with $1$s on the diagonal, and $D$ is diagonal with positive diagonal entries. Show that the factorization is unique: If $A = L_{1}D_{1}U_{1}$ is another such factorization, show that $L_{1} = L$, $D_{1} = D$, and $U_{1} = U$.

\item Show that a matrix $A$ is positive definite if and only if $A$ is symmetric and admits a factorization $A = LDU$ as in (a).
\end{enumerate}
\begin{hint}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  If $A$ is positive definite, use Theorem~\ref{thm:024815} to write $A = U^{T}U$ where $U$ is upper triangular with positive diagonal $D$. Then $A = (D^{-1}U)^{T}D^{2}(D^{-1}U)$ so $A = L_{1}D_{1}U_{1}$ is such a factorization if $U_{1} = D^{-1}U$, $D_{1} = D^{2}$, and $L_{1} = U^T_1$. Conversely, let $A^{T} = A = LDU$ be such a factorization. Then $U^{T}D^{T}L^{T} = A^{T} = A = LDU$, so $L = U^{T}$ by \textbf{(a)}. Hence $A = LDL^{T} = V^{T}V$ where $V = LD_{0}$ and $D_{0}$ is diagonal with $D^2_0 = D$ (the matrix $D_{0}$ exists because $D$ has positive diagonal entries). Hence $A$ is symmetric, and it is positive definite by Example~\ref{exa:024865}.

\end{enumerate}
\end{hint}
\end{problem}


\begin{problem}\label{prob:pos_def_13}
Let $A$ be positive definite and write $d_{r} = \mbox{det}{^{(r)}A}$ for each $r = 1, 2, \dots, n$. If $U$ is the upper triangular matrix obtained in step 1 of the algorithm, show that the diagonal elements $u_{11}, u_{22}, \dots, u_{nn}$ of $U$ are given by $u_{11} = d_{1}$, $u_{jj} = d_{j} / d_{j-1}$ if $j > 1$. [\textit{Hint}: If $LA = U$ where $L$ is lower triangular with $1$s on the diagonal, use block multiplication to show that $\mbox{det}{^{(r)}A} = \mbox{det}{^{(r)}U}$ for each $r$.]
\end{problem}

\end{document} 